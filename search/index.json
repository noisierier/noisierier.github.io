[{"content":"课程介绍 信息论（2019年春）台湾交通大学陈伯宁，哔哩哔哩视频：信息论（2019年春）台湾交通大学陈伯宁,参考网站：消息理論,参考书籍：An Introduction to Single-User Information Theory。该课程将从公理出发，严格的证明香农三大定律，探索信息论的方法论。\nIntroduction 该章节不从严格的数学角度描述，而是从一个 Overview 的角度宏观的了解香农的工作。\n信息从使用 code 表示开始，什么样的 code 是最好的呢，code 的紧凑程度(Compactness)是一种很好的度量，基于统计的视角，即平均比特长度可以度量code的紧凑长度，如何去度量使用code表示的信息多少呢，信息如何测度呢，香农从三大公理开始建立信息论的数学基础：\nMonotonicity in event probability.发生概率较小的事件的信息量大，带给接收方的冲击也大，发生概率较大的事件信息量少。\nAdditivity.两个事件的联合信息量必然不少于单独一个事件的信息量。\nContinuity.微小的事件概率变化引起的信息量变化也是微小的。\n这三条公理也对应着人类对于世界的认知:我们所生活的世界是温和的。就如同高斯分布是满足熵最大分布的原理一样，符合现实世界的分布必然不是重尾的，而是测度集中的。上述三条公理将导出唯一的可以度量信息的函数： $$ \\text { self-information of an event }=\\log _2 \\frac{1}{\\text { event probability }} \\text { bits. } $$ 该公式与物理中熵的定义不谋而合，有故事称香农一开始并不是给该信息量命名为信息熵，是香农的老师建议，才将该度量命名为信息熵。\n我们假设信息以一定的处理方式在发送端进行处理: 现在有了信息的度量，那么是否可以度量出一种code，可以最紧凑的表达信息，香农第一定理便是回答了这个问题： $$\\text{Min Compression rate} = \\text{entropy of } (Z_1,Z_2,\\cdots,Z_n)$$ 香农虽然回答了该问题，说明信息的压缩有一个极限，但是并没有给出具体的编码方式，直到50年之后turbo码的出现才第一次逼近了这个极限。值得注意的是，在逼近这个极限时，压缩编码$U_1,\\cdots,U_n$是渐进均匀分布的，且是 i.i.d.的随机变量，该定理也进一步方便了后续的信道编码处理与分析。\n香农第二定律研究了信息在含有噪声的信道下传输的特性，从很多领域中我们都可以看到可靠传输这样的概念，但是真正严谨的可靠性传输并不是指0差错，而是可以用 $\\epsilon-\\sigma$ 语言去描述的渐进无差错概率，而差错不可能是0，因为香农研究在大样本背景下的编码和通信。\n所以对于信源将原始信息 $Z$ 压缩为的 $U$,我们必须进一步将其编码，添加冗余，使其在信道中即使受了噪声影响，通过接收端的解码也可以恢复原来的信息，这就是Channel encoder的作用，一般而言，信源和信道编码器都是分开研究的，不过也有学者将二者放在一起进行研究。\n那么是否真的有这样一种编码，可以让差错概率渐进为0吗，香农第二定律回答了该问题：$$ \\text{Channel capacity is the max reliable transmission code rate for a noisy channel.} $$ 香农是从数学的角度定义了互信息，然后证明该定理的，但是与第一定律一样，并没有得出具体的编码形式，而是证明了确有编码可以达到该效果。（这里的描述是不严谨的，但是可以方便我们很快的了解香农的工作）\n至于香农第三定律，需要引入更多的名词才能介绍。\n香农的信息论并不只是简单的这三条定理，其定理证明的背后也蕴含着一套方法论，就是不找出具体的方法，但是从数学上证明确有此事，这样的方法论也融合到了很多的学科之中。\nOverview on Suprema and Limits Overview in Probability and Random Processes Information Measures for Discrete Systems Lossless Data Compression Data Transmission and Channel Capacity Differential Entropy and Gaussian Channels Lossy Data Compression and Transmission ","date":"2025-02-18T00:21:05+08:00","image":"http://localhost:1313/post_file/%E4%BF%A1%E6%81%AF%E8%AE%BA/cover.png","permalink":"http://localhost:1313/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BF%A1%E6%81%AF%E8%AE%BA/","title":"学习笔记|信息论"},{"content":"课程介绍 统计研究院陳鄰安老师数理统计，参考网站：统计学1，统计学2;youtube课程：交大數理統計學，陳鄰安老師 - 高等統計學。\nFundamentals of probability theory Random Variable Define 1(random variable). Let $(\\Omega, \\mathcal{F}, P)$ be a probability space and $(E, \\mathcal{E})$ a measurable space. Then an $(E, \\mathcal{E})$-valued random variable is a measurable function $X: \\Omega \\rightarrow E$. Remark.\n随机变量是从样本空间到$\\mathbb{R}$的映射，概率测度是从$\\sigma$-域到$[0,1]$的映射，并且满足整个样本空间的测度等于1。在实际中我们一般不会考虑原来的样本空间，因为一般考虑由随机变量诱导出来的概率测度，它是由博雷尔集($\\mathcal{B}$)到$\\mathbb{R}$的映射。 $\\forall B\\in \\mathcal{B},X^{-1}(B)\\in \\mathcal{F},\\text{where} X^{-1}(B) = \\{\\omega \\in \\Omega\\mid X(\\omega)\\in B\\}$,which means that, for every subset $B \\in \\mathcal{E}$, its preimage is $\\mathcal{F}$-measurable; $X^{-1}(B) \\in \\mathcal{F}$, where $X^{-1}(B)=\\{\\omega: X(\\omega) \\in B\\}$. This definition enables us to measure any subset $B \\in \\mathcal{E}$ in the target space by looking at its preimage, which by assumption is measurable. 由随机变量很容易定义累积分布函数(CDF)： $$ F(x) = P(X(\\omega) \\leq x) = P(\\{\\omega\\in\\Omega:X(\\omega)\\in (-\\infty,x]\\}) $$ Def 2(m.g.f.). Let $X$ be a random variable with CDF $F_X$. The moment generating function (mgf) of $X$ (or $F_X$ ), denoted by $M_X(t)$, is $$ M_X(t)=\\mathrm{E}\\left[e^{t X}\\right] $$ provided this expectation exists for $t$ in some open neighborhood of 0 . That is, there is an $h\u0026gt;0$ such that for all $t$ in $-h\u0026lt;t\u0026lt;h, \\mathrm{E}\\left[e^{t X}\\right]$ exists. If the expectation does not exist in an open neighborhood of 0 , we say that the moment generating function does not exist. In other words, the moment-generating function of $X$ is the expectation of the random variable $e^{t X}$. More generally, when $\\mathbf{X}=\\left(X_1, \\ldots, X_n\\right)^{\\mathrm{T}}$, an $n$-dimensional random vector, and $\\mathbf{t}$ is a fixed vector, one uses $\\mathbf{t} \\cdot \\mathbf{X}=\\mathbf{t}^{\\mathrm{T}} \\mathbf{X}$ instead of $t X$ : $$ M_{\\mathbf{X}}(\\mathbf{t}):=\\mathrm{E}\\left(e^{\\mathbf{t}^{\\mathrm{T}} \\mathbf{X}}\\right) $$ $M_X(0)$ always exists and is equal to 1 . Note. 几种常见分布的 m.g.f: $$ \\begin{cases}X \\sim N\\left(\\mu, \\sigma^2\\right) \u0026amp; , M_x(t)=e^{\\mu t+\\frac{\\sigma^2}{2} t^2}, \\forall t \\in R \\\\ X \\sim \\operatorname{Gamma}(\\alpha, \\beta) \u0026amp; , M_x(t)=(1-\\beta t)^{-\\alpha}, t\u0026lt;\\frac{1}{\\beta} \\\\ X \\sim b(n, p) \u0026amp; , M_x(t)=\\left(1-p+p e^t\\right)^n, \\forall t \\in R \\\\ X \\sim \\operatorname{Poisson}(\\lambda) \u0026amp; , M_x(t)=e^{\\lambda\\left(e^t-1\\right)}, \\forall t \\in R\\end{cases} $$\nLemma 3. 若$X$为r.v.,令g为$\\mathbb{R}\\rightarrow\\mathbb{R}$的映射，则$Y=g(X)$同样为r.v.,且可以确定其分布。\nRemark.\n两种求得$g(X)$的pdf的方法：\nDistribution method:\nSuppose that X is a continuous r.v.. Let $Y=g(X)$ The d.f(distribution function) of Y is $$ G(y)=P(Y \\leq y)=P(g(X) \\leq y) $$ If G is differentiable then the p.d.f. of $Y=g(X)$ is $g(y)=G^{\\prime}(y)$.\nmgf method :(moment generating function) $$ \\begin{aligned} E\\left[e^{t x}\\right]= \\begin{cases}\\sum e^{t x} f(x) \u0026amp; \\text { (discrete) } \\\\ \\int_{-\\infty}^{\\infty} e^{t x} f(x) d x \u0026amp; \\text { (continuous) }\\end{cases} \\end{aligned} $$\nThm 4. m.g.f. $M_x(t)$ and its distribution (p.d.f. or d.f.) forms a $1-1$ functions.\nThm 5. $\\quad M_X^{(k)}(0)=E\\left[X^k\\right], k=1,2, \\cdots$\nProbability distribution Def 1. Gamma function $\\Gamma(\\alpha)=\\int_0^{\\infty} x^{\\alpha-1} e^{-x} d x$\nProperties:\n$\\Gamma(\\alpha)=(\\alpha-1) \\Gamma(\\alpha-1)$, if $\\alpha\u0026gt;1$ $\\Gamma(1)=\\int_0^{\\infty} e^{-x} d x=1$ $\\Gamma(n)=(n-1) \\Gamma(n-1)=\\cdots=(n-1)(n-2) \\cdots 1 \\cdot \\Gamma(1)=(n-1)$ ! $\\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi}$ Def 2. We say that $X$ has a Gamma distribution if it has p.d.f $$ f(x)=\\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha-1} e^{-\\frac{x}{\\beta}}, x\u0026gt;0, \\text { for some } \\alpha\u0026gt;0, \\beta\u0026gt;0 $$ We denote by $X \\sim \\operatorname{Gamma}(\\alpha, \\beta)$.\nNote. If $X$ has Gamma distribution with $\\beta=2$ and $\\alpha=\\frac{r}{2}$, we say that $X$ has a chi-square distribution with degrees of freedom r. The p.d.f is $$ f(x)=\\frac{1}{\\Gamma\\left(\\frac{r}{2}\\right) 2^{\\frac{r}{2}}} x^{\\frac{r}{2}-1} e^{-\\frac{x}{2}}, x\u0026gt;0 $$ We denote by $X \\sim \\chi^2(r)$.\nEx 1. If $X \\sim \\operatorname{Gamma}(\\alpha, \\beta)$, m.g.f of $X$ is\n$$ \\begin{aligned} M_X(t) \u0026amp; =E\\left[e^{t X}\\right]=\\int_0^{\\infty} e^{t x} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha-1} e^{-\\frac{x}{\\beta}} d x=\\int_0^{\\infty} \\frac{1}{\\Gamma(\\alpha) \\beta^\\alpha} x^{\\alpha-1} e^{-\\frac{(1-\\beta t) x}{\\beta}} d x \\\\ \u0026amp; =(1-\\beta t)^{-\\alpha} \\int_0^{\\infty} \\frac{1}{\\Gamma(\\alpha)\\left(\\frac{\\beta}{1-\\beta t}\\right)^2} x^{\\alpha-1} e^{-\\frac{x}{1-\\beta t}} d x \\\\ \u0026amp; =(1-\\beta t)^{-\\alpha}, t\u0026lt;\\frac{1}{\\beta} \\\\ \u0026amp; \\because \\frac{\\beta}{1-\\beta t}\u0026gt;0 \\Rightarrow 1-\\beta t\u0026gt;0 \\Rightarrow t\u0026lt;\\frac{1}{\\beta} \\\\ M_X^{\\prime}(t) \u0026amp; =\\alpha(1-\\beta t)^{-\\alpha-1} \\beta \\\\ \u0026amp; \\Rightarrow \\operatorname{Mean} \\mu=E[X]=M_X^{\\prime}(0)=\\alpha \\beta \\\\ M_X^{\\prime \\prime}(t) \u0026amp; =\\alpha(\\alpha+1)(1-\\beta t)^{-\\alpha-2} \\beta^2 \\\\ \u0026amp; \\Rightarrow E\\left[X^2\\right]=M_X^{\\prime \\prime}(0)=\\alpha(\\alpha+1) \\beta^2 \\\\ \u0026amp; \\Rightarrow \\text { Variance } \\sigma^2=M_X^{\\prime \\prime}(0)-\\left(M_X^{\\prime}(0)\\right)^2=\\alpha(\\alpha+1) \\beta^2-(\\alpha \\beta)^2=\\alpha \\beta^2 \\end{aligned} $$\nRandom Voctor Definition 1. Suppose that the random variables $X$ and $Y$ are defined to assume values in $\\displaystyle I\\subseteq \\mathbb {R}$. We say that $n$ random variables $X_1, \\ldots, X_n$ are i.i.d(Independent and identically distributed random variables). If and only if\n$F_{X_1}(x)=F_{X_k}(x)$ , $\\forall k \\in{1, \\ldots, n}$ and $\\forall x \\in I$ $F_{X_1, \\ldots, X_n}\\left(x_1, \\ldots, x_n\\right)=F_{X_1}\\left(x_1\\right) \\cdot \\ldots \\cdot F_{X_n}\\left(x_n\\right)$ , $\\forall x_1, \\ldots, x_n \\in I$ where $F_{X_1, \\ldots, X_n}\\left(x_1, \\ldots, x_n\\right)=\\mathrm{P}\\left(X_1 \\leq x_1 \\wedge \\ldots \\wedge X_n \\leq x_n\\right)$ denotes the joint cumulative distribution function of $X_1, \\ldots, X_n$\nThm 2(Theory for change variables). $$ P\\left(\\left(\\begin{array}{c} x_1 \\\\ \\vdots \\\\ x_n \\end{array}\\right) \\in A\\right)=\\int \\cdots \\int f_{X_1, \\ldots, X_n}\\left(x_1, \\ldots, x_n\\right) d x_1 \\cdots d x_n $$ Let $y_1=g_1\\left(x_1, \\ldots, x_n\\right), \\cdots, y_n=g_n\\left(x_1, \\ldots, x_n\\right)$ be a $1-1$ function with inverse $x_1=w_1\\left(y_1, \\ldots, y_n\\right), \\cdots, x_n=w_n\\left(y_1, \\ldots, y_n\\right)$ and Jacobian $$ J=\\left|\\begin{array}{ccr} \\frac{\\partial x_1}{\\partial y_1} \u0026amp; \\cdots \u0026amp; \\frac{\\partial x_1}{\\partial y_n} \\\\ \\vdots \u0026amp; \u0026amp; \\vdots \\\\ \\frac{\\partial x_n}{\\partial y_1} \u0026amp; \\cdots \u0026amp; \\frac{\\partial x_n}{\\partial y_n} \\end{array}\\right| $$ Then $$ \\begin{aligned} \u0026amp; \\int \\cdots \\int f_{X_1, \\ldots, X_n}\\left(x_1, \\ldots, x_n\\right) d x_1 \\cdots d x_n \\\\ \u0026amp; =\\int \\cdots \\int f_{X_1, \\ldots, X_n}\\left(w_1\\left(y_1, \\ldots, y_n\\right), \\ldots, w_n\\left(y_1, \\ldots, y_n\\right)\\right)|J| d y_1 \\cdots d y_n \\end{aligned} $$ Hence, joint p.d.f. of $Y_1, \\cdots, Y_n$ is $$ f_{Y_1, \\ldots, Y_n}\\left(y_1, \\ldots, y_n\\right)=f_{X_1, \\ldots, X_n}\\left(w_1, \\ldots, w_n\\right)|J| $$ Remark. 可以用该方法构造从$n$个随机变量的联合分布到另外$n$个随机变量联合分布的映射(注意映射到的值域)， 然后再通过积分的方式获得边缘联合分布。\nStatistical basis Def 1(随机样本). If a sequence of r.v.\u0026rsquo;s $X_1,\\cdots,X_n$ are i.i.d.,then they are called a random sample.\nDef 2(统计量和参数). Any function $g(X_1,\\cdots,X_n)$ of a random sample $X_1,\\cdots,X_n$ which is not dependent on a parameter $\\theta$ is called a statistic.\nNote. If $X$ is a random sample with p.d.f. $f(x;\\theta)$, where $\\theta$ is an unknown constant, then $\\theta$ is called parameter.统计量是一个可观察的随机变量，这是它与统计参数的直接区别 ，统计参数包含一个不可观察的随机变量从而可以描述统计总体。 统计参数必须要在整体数据都可被观察的时候才能计算，例如，一个完美的人口普查。\n为了介绍 Thm 8,我们介绍以下定义和定理作为铺垫，这些定理同样是很实用的定理。\nLemma 2(独立的等价判定). $X_1$ and $X_2$ are independent if and only if $$ M_{X_1, X_2}\\left(t_1, t_2\\right)=M_{X_1}\\left(t_1\\right) M_{X_2}\\left(t_2\\right), \\forall t_1, t_2 . $$\nThm 3. 关于独立的两个定理：\nIf $\\left(X_1, \\ldots, X_n\\right)$ and $\\left(Y_1, \\ldots, Y_m\\right)$ are independent（that is:$f(X_1, \\ldots, X_n,Y_1, \\ldots, Y_m) = f(X_1, \\ldots, X_n)f(Y_1, \\ldots, Y_m)$）, then $g\\left(X_1, \\ldots, X_n\\right)$ and $h\\left(Y_1, \\ldots, Y_m\\right)$ are also independent. If $X, Y$ are independent, then $$ \\mathrm{E}[g(X) h(Y)]=\\mathrm{E}[g(X)] \\mathrm{E}[h(Y)] $$ Thm 4. Joint m.g.f. $M_{X,Y}(0,t) = M_Y(t)$.\nThm 5. $X \\sim \\chi^2(r_1)$,$Y \\sim \\chi^2(r_2)$,if $X\\perp Y$,then $X+Y\\sim \\chi^2(r_1+r_2)$.\nThm 6. $Z\\sim N(0,1)$,then $Z^2\\sim \\chi^2(1)$.\nDef 7(样本均值，样本方差). 一组随机样本 $X_1,\\ldots,X_n$,样本均值为 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$，样本方差为 $S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\bar{X})^2$.二者都是统计量。\nThm 8. If $\\left(X_1, \\ldots, X_n\\right)$ is a random sample from $N\\left(\\mu, \\sigma^2\\right)$, then\n(a) $\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)$\n(b) $\\bar{X}$ and $S^2$ are independent.\n(c) $\\frac{(n-1) S^2}{\\sigma^2} \\sim \\chi^2(n-1)$ Remark.\n样本均值不等于均值，样本均值的期望才等于均值，注意区分以下两者的关系。 $$ \\begin{cases} \\sum_i \\left( \\frac{X_i-\\mu}{\\sigma}\\right)^2\\sim\\chi^2(n) \\\\ \\frac{(n-1) S^2}{\\sigma^2} = \\sum_i \\left( \\frac{X_i-\\bar{X}}{\\sigma}\\right)^2 \\sim \\chi^2(n-1) \\end{cases} $$ Thm 8 的(b)是十分难得的，因为二者都可以看作 $X_i$ 的函数，而存在独立性， 归根结底是因为 $\\bar{X}$ 和 $\\left(X_1-\\bar{X},\\ldots,X_n-\\bar{X}\\right)$ 独立， 即 $$M_{\\bar{X}}(t)M_{X_1-\\bar{X},\\ldots,X_n-\\bar{X}}(t_1,\\ldots,t_n) = M_{\\bar{X},X_1-\\bar{X},\\ldots,X_n-\\bar{X}}(t,t_1,\\ldots,t_n)$$. Problem in statistics:\nA random variables $X$ with p.d.f. of the form $f(x, \\theta)$ where function $f$ is known but parameter $\\theta$ is unknown. We want to gain knowledge about $\\theta$.\nWhat we have for inference:\nThere is a random sample $X_1, \\ldots, X_n$ from $f(x, \\theta)$. $$ \\text { Statistical inferences }\\left\\{\\begin{array}{l} \\text { Estimation }\\left\\{\\begin{array}{l} \\text { Point estimation: } \\hat{\\theta}=\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right) \\\\ \\text { Interval estimation: } \\\\ \\text { Find statistics } T_1=t_1\\left(X_1, \\ldots, X_n\\right), T_2=t_2\\left(X_1, \\ldots, X_n\\right) \\\\ \\text { such that } 1-\\alpha=P\\left(T_1 \\leq \\theta \\leq T_2\\right) \\end{array}\\right. \\\\ \\text { Hypothesis testing: } H_0: \\theta=\\theta_0 \\text { or } H_0: \\theta \\geq \\theta_0 . \\\\ \\text { Want to find a rule to decide if we accept or reject } H_0 . \\end{array}\\right. $$\nStatistical Inference - Point Estimation Unbiased and consistent Def 1（估计量）. We call a statistic $\\hat{\\theta}=\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right)$ an estimator of parameter $\\theta$ if it is used to estimate $\\theta$. If $X_1=x_1, \\ldots, X_n=x_n$ are observed, then $\\hat{\\theta}=\\hat{\\theta}\\left(x_1, \\ldots, x_n\\right)$ is called an estimate of $\\theta$.\nDef 2(无偏估计). We call an estimator $\\theta$ unbiased for $\\theta$ if it satisfies $$ \\begin{gathered} E_\\theta\\left(\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right)\\right)=\\theta, \\forall \\theta \\\\ \\mathrm{E}_\\theta\\left(\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right)\\right)=\\left\\{\\begin{array}{l} \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\hat{\\theta}\\left(x_1, \\ldots, x_n\\right) f\\left(x_1, \\ldots, x_n, \\theta\\right) d x_1 \\cdots d x_n \\\\ \\int_{-\\infty}^{\\infty} \\theta^* f_{\\hat{\\theta}}\\left(\\theta^*\\right) d \\theta^* \\text { where } \\hat{\\theta}=\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right) \\text { is a r.v. with pdf } f_{\\hat{\\theta}}\\left(\\theta^*\\right) \\end{array}\\right. \\end{gathered} $$\nRemark. $\\bar{X}$ 和 $S^2$ 都是无偏估计量。\n既然是点估计，我们有必要选择一种度量来衡量点估计是否准确(不可以选择概率作为度量，因为对于连续分布来说，统计量等于任意一点的概率为0，没有意义)。\nDef 3（依概率收敛）. We say that $X_n$ converges to $X$,(a r.v. or a constant) in probability if for $\\epsilon\u0026gt;0$, $$ P\\left(\\left|X_n-X\\right|\u0026gt;\\epsilon\\right) \\longrightarrow 0, \\text { as } n \\longrightarrow \\infty $$ In this case, we denote $X_n \\xrightarrow{P} X$.\nRemark. 该定理的意思是，$\\{\\omega\\in\\Omega:|X_n(\\omega)-X(\\omega)|\u0026lt;\\epsilon\\}$这个事件的概率测度收敛到1.不加证明的说明一个定理:对于连续函数 $g$,若$X_n \\xrightarrow{P} X$，则$g(X_n) \\xrightarrow{P} g(X)$.\n下面引入两种容易判断随机变量依概率收敛的定理，在引入定理之前，回顾一下集中不等式。\nThm 4（马尔可夫不等式）. If $X$ is a nonnegative random variable and $a\u0026gt;0$, then the probability that $X$ is at least $a$ is at most the expectation of $X$ divided by a: $$ \\mathrm{P}(X \\geq a) \\leq \\frac{\\mathrm{E}(X)}{a} $$ When $\\mathrm{E}(X)\u0026gt;0$, we can take $a=\\tilde{a} \\cdot \\mathrm{E}(X)$ for $\\tilde{a}\u0026gt;0$ to rewrite the previous inequality as $$ \\mathrm{P}(X \\geq \\tilde{a} \\cdot \\mathrm{E}(X)) \\leq \\frac{1}{\\tilde{a}} $$\nThm 5(判断依概率收敛). If $E\\left(X_n\\right)=a$ or $E\\left(X_n\\right) \\longrightarrow a$ and $\\operatorname{Var}\\left(X_n\\right) \\longrightarrow 0$ when $n\\rightarrow\\infty$, then $X_n \\xrightarrow{P} a$.\nProof. $$ \\begin{aligned} \\mathrm{E}\\left[\\left(X_n-a\\right)^2\\right] \u0026amp; =\\mathrm{E}\\left[\\left(X_n-\\mathrm{E}\\left(X_n\\right)+\\mathrm{E}\\left(X_n\\right)-a\\right)^2\\right] \\\\ \u0026amp; =\\mathrm{E}\\left[\\left(X_n-\\mathrm{E}\\left(X_n\\right)\\right)^2\\right]+\\mathrm{E}\\left[\\left(\\mathrm{E}\\left(X_n\\right)-a\\right)^2\\right]+2 \\mathrm{E}\\left[\\left(X_n-\\mathrm{E}\\left(X_n\\right)\\right)\\left(\\mathrm{E}\\left(X_n\\right)-a\\right)\\right] \\\\ \u0026amp; =\\operatorname{Var}\\left(X_n\\right)+\\mathrm{E}\\left(\\left(X_n\\right)-a\\right)^2 \\end{aligned} $$ By Chebyshev\u0026rsquo;s Inequality(可以由马尔可夫不等式得到) : $$ P\\left(\\left|X_n-X\\right| \\geq \\epsilon\\right) \\leq \\frac{\\mathrm{E}\\left(X_n-X\\right)^2}{\\epsilon^2} \\\\ \\text { or } P\\left(\\left|X_n-\\mu\\right| \\geq k \\sigma\\right) \\leq \\frac{1}{k^2} $$ For $\\epsilon\u0026gt;0$, $$ \\begin{aligned} \u0026amp; 0 \\leq P\\left(\\left|X_n-a\\right|\u0026gt;\\epsilon\\right)=P\\left(\\left(X_n-a\\right)^2\u0026gt;\\epsilon^2\\right) \\\\ \u0026amp; \\leq \\frac{\\mathrm{E}\\left(X_n-a\\right)^2}{\\epsilon^2}=\\frac{\\operatorname{Var}\\left(X_n\\right)+\\left(\\mathrm{E}\\left(X_n\\right)-a\\right)^2}{\\epsilon^2} \\longrightarrow 0 \\text { as } n \\longrightarrow \\infty . (\\text{条件})\\\\ \u0026amp; \\Rightarrow P\\left(\\left|X_n-a\\right|\u0026gt;\\epsilon\\right) \\longrightarrow 0, \\text { as } n \\longrightarrow \\infty . \\Rightarrow X_n \\xrightarrow{P} a . \\end{aligned} $$ Q.E.D.\nRemark. 上述定理中 $X_n$ 是统计量，我们同样可以用 $\\hat{\\theta}$ 将其表达，通过该定理，可以容易的判断统计量是否依概率收敛于参数。\nThm 6(Weak Law of Large Numbers,WLLN). If $X_1, \\ldots, X_n$ is a random sample with mean $\\mu$ and finite variance $\\sigma^2$, then $\\bar{X} \\xrightarrow{P} \\mu$.\nProof. $$ \\mathrm{E}(\\bar{X})=\\mu, \\operatorname{Var}(\\bar{X})=\\frac{\\sigma^2}{n} \\longrightarrow 0 \\text { as } n \\longrightarrow \\infty . \\Rightarrow \\bar{X} \\xrightarrow{P} \\mu $$ Q.E.D.\nRemark. 上述定理中的 $X_1, \\ldots, X_n$同样可以写为 $X_1^k, \\ldots, X_n^k$,则可以得到，若 $Var(X^k)$ 有限，则 $\\bar{X^k} \\xrightarrow{P} \\mathbb{E}X^k$.\n对于满足依概率收敛的统计量，有以下定义：\nDef 7(一致估计量). We sat that $\\hat{\\theta}$ is a consistent estimator of $\\theta$ if $\\hat{\\theta} \\xrightarrow{P} \\theta$ .\nRemark. $\\bar{X}和S^2$分别是 $\\mu$ 和 $\\sigma^2$一致估计量.\nDef 8(Moments): Let $X$ be a random variable having a p.d.f. $f(x, \\theta)$, the population $k_{t h}$ moment is defined by $$ \\mathrm{E}_\\theta\\left(X^k\\right)= \\begin{cases}\\sum_{\\text {all } x} x^k f(x, \\theta) \u0026amp; , \\text { discrete } \\\\ \\int_{-\\infty}^{\\infty} x^k f(x, \\theta) d x \u0026amp; , \\text { continuous }\\end{cases} $$ The sample $k_{t h}$ moment is defined by $\\frac{1}{n} \\sum_{i=1}^n X_i{ }^k$.\nRemark. 假设方差有限，样本原点矩是原点矩的无偏估计量和一致性估计量 (WLLN)。\nPoint estimation method Method 1:矩估计，Method of Moments, MoM\n使用样本矩估计原点矩，然后反解出参数 $\\theta$ 作为 $\\hat{\\theta}$。\nIf $\\theta=\\left(\\theta_1, \\ldots, \\theta_k\\right)$ is $k$-variate, the method of moment estimator $\\left(\\hat{\\theta_1}, \\ldots, \\hat{\\theta_k}\\right)$ solves $\\theta_1, \\ldots, \\theta_k$ for $$ \\frac{1}{n} \\sum_{i=1}^n X_i{ }^j=E_{\\theta_1, \\ldots, \\theta_k}\\left(X^j\\right), j=1, \\ldots, k $$ Remark. 矩估计方法可以保证估计量相对于k阶原点矩是无偏且一致的，但无法保证中心矩的无偏性和一致性。\nMethod 2:,最大似然估计，Maximum Likelihood Estimator\nLet $X_1, \\ldots, X_n$ be a random sample with p.d.f. $f(x, \\theta)$. The joint p.d.f. of $X_1, \\ldots, X_n$ is $$ f\\left(x_1, \\ldots, x_n, \\theta\\right)=\\prod_{i=1}^n f\\left(x_i, \\theta\\right), x_i \\in R, i=1, \\ldots, n $$ Let $\\Theta$ be the space of possible values of $\\theta$. We call $\\Theta$ the parameter space.\nDef 1(似然函数). The likelihood function of a random sample is defined as its joint p.d.f. as $$ L(\\theta)=L\\left(\\theta, x_1, \\ldots, x_n\\right)=f\\left(x_1, \\ldots, x_n, \\theta\\right), \\theta \\in \\Theta $$ which is considered as a function of $\\theta$. Remark. 这里穿插一点贝叶斯学派和频率学派不同的一点，频率学派认为 $\\theta$ 是一个确定的数值，尽管难以得到，但是这个数值是确定的，而分布一般写成 $F(X_1,\\cdots,X_N,\\theta)$,这是为了强调 $\\theta$ 作为函数的参数这一点，但是按照严格的概率论写法，括号内应为随机变量；贝叶斯学派认为 $\\theta$ 本身就是服从一定分布的随机变量，一般将似然函数写为 $L(\\theta) = f(X_1,X_2,\\cdots,X_n\\mid \\theta)$,写法的不同反应了对参数空间认知的根本差异。\nDef 2(似然估计量). Let $\\hat{\\theta}=\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right)$ be any value of $\\theta$ that maximizes $L\\left(\\theta, X_1, \\ldots, X_n\\right)$. Then we call $\\hat{\\theta}=\\hat{\\theta}\\left(X_1, \\ldots, X_n\\right)$ the maximum likelihood estimator (m.l.e) of $\\theta$. When $X_1=x_1, \\ldots, X_n=x_n$ is observed, we call $\\hat{\\theta}=\\hat{\\theta}\\left(x_1, \\ldots, x_n\\right)$ the maximum likelihood estimate of $\\theta$.\nRemark. 即似然估计量为 $$ \\hat{\\theta} = \\argmax_{\\theta\\in\\Theta}L(\\theta) $$ 理解最大似然估计有很多种方法，一种最直观的方法就是使得当前采样事件发生概率最大的参数就是最大似然估计量需要估计的参数，可以证明，最大似然估计量可以等价表示为(或者具有如下性质) $$ \\hat{\\theta} =\\argmin_{\\theta\\in\\Theta} D_{k L}\\left(f_\\theta(x) \\| f_{\\hat{\\theta}}(x)\\right) $$ 其中,$f_\\theta(x) $ 和 $f_{\\hat{\\theta}}(x)$ 分别表示真实分布和估计分布。 Remark 2. 凡所发生，皆应发生。最大似然估计就是找到使得当前事件发生概率最大的估计量。\n在给出具体例子之前，我们引入顺序统计量的概念和一个定理：\nDef 3(Order statistics). Let $\\left(X_1, \\ldots, X_n\\right)$ be a random sample with d.f. F and p.d.f. f. Let $\\left(Y_1, \\ldots, Y_n\\right)$ be a permutation $\\left(X_1, \\ldots, X_n\\right)$ such that $Y_1 \\leq Y_2 \\leq \\cdots Y_n$. Then we call $\\left(Y_1, \\ldots, Y_n\\right)$ the order statistic of $\\left(X_1, \\ldots, X_n\\right)$ where $Y_1$ is the first (smallest) order statistic, $Y_2$ is the second order statistic,\u0026hellip;, $Y_n$ is the largest order statistic.\nRemark. $Y_1 = \\min\\{X_1, \\ldots, X_n\\}$,$Y_n = \\max\\{X_1, \\ldots, X_n\\}$;顺序统计量一般不相互独立，也不同分布。\nThm 4(两个顺序统计量的pdf). Let $\\left(X_1, \\ldots, X_n\\right)$ be a random sample from a \u0026ldquo;continuous distribution\u0026rdquo; with p.d.f. $f(x)$ and d.f $F(x)$. Then the p.d.f. of $Y_n=\\max \\left\\{X_1, \\ldots, X_n\\right\\}$ is $$ g_n(y)=n(F(y))^{n-1} f(y) $$ and the p.d.f. of $Y_1=\\min \\left\\{X_1, \\ldots, X_n\\right\\}$ is $$ g_1(y)=n(1-F(y))^{n-1} f(y) $$\nContinue to Point Estimation-UMVUE Confidence Interval ","date":"2025-01-26T00:21:05+08:00","image":"http://localhost:1313/post_file/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/cover.png","permalink":"http://localhost:1313/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","title":"学习笔记|数理统计"},{"content":" 课程介绍 谢启鸿老师高等代数学，参考网站：谢启鸿高等代数官方博客,b站课程：数学专业 高等代数学-复旦大学-谢启鸿-高清。\n第一章 行列式 可以由递归形式方便的定义n阶行列式，并且定义余子式和代数余子式。下面先介绍行列式的一些性质：\n性质1 上下三角行列式的值等于其对角线元素之积。\n性质2 行列式的某行或某列全为零，则行列式等于零。\n性质3 用常数$c$乘行列式的某一列或某一行，得到的行列式的值等于原行列式的$c$倍。\n性质4 交换行列式不同的两行(列)，行列式的值改变符号。\n性质5 若行列式两行或两列成比例，则行列式的值等于零。特别，若行列式两行或两列相同，则行列式的值等于零。\n性质6 若行列式中某行(列)元素均为两项之和，则行列式可以表示为两个行列式之和。\n性质7 行列式的某一行(列)乘以某个数加到另外一行(列)上，行列式的值不变。\n性质8 行列式和其转置具有相同的值。\n定理1 行列式可以按照任意一行或任意一列的形式进行展开，如果线性组合的行(列)和对应的余子式(代数余子式)不匹配，计算结果等于零。\n定理2(Cramer法则) 设有线性方程组\n$$ \\left\\{\\begin{array}{c} a_{11} x_1+a_{12} x_2+\\cdots+a_{1 n} x_n=b_1 \\\\ a_{21} x_1+a_{22} x_2+\\cdots+a_{2 n} x_n=b_2 \\\\ \\cdots \\cdots \\cdots \\\\ a_{n 1} x_1+a_{n 2} x_2+\\cdots+a_{n n} x_n=b_n \\end{array}\\right. $$ 记这个方程组的系数行列式为 $|\\boldsymbol{A}|$ ，若 $|\\boldsymbol{A}| \\neq 0$ ，则方程组有且仅有一组解： $$ x_1=\\frac{\\left|A_1\\right|}{|\\boldsymbol{A}|}, x_2=\\frac{\\left|A_2\\right|}{|\\boldsymbol{A}|}, \\cdots, x_n=\\frac{\\left|A_n\\right|}{|\\boldsymbol{A}|} $$ 其中 $\\left|\\boldsymbol{A}_{\\boldsymbol{j}}\\right|(j=1,2, \\cdots, n)$ 是一个 $n$ 阶行列式，它由 $|\\boldsymbol{A}|$ 去掉第 $j$ 列换上方程组的常数项 $b_1, b_2, \\cdots, b_n$ 组成的列而成． 例1 对于行列式的计算，优先利用性质将行列式进行降阶，构造出0和1。特殊的，对于Vandermonde行列式：\n$$ \\begin{aligned} V_n=\\left|\\begin{array}{ccccc} 1 \u0026 x_1 \u0026 x_1^2 \u0026 \\cdots \u0026 x_1^{n-1} \\\\ 1 \u0026 x_2 \u0026 x_2^2 \u0026 \\cdots \u0026 x_2^{n-1} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \u0026 \\\\ 1 \u0026 x_n \u0026 x_n^2 \u0026 \\cdots \u0026 x_n^{n-1} \\end{array}\\right|=\\prod_{1 \\leqslant i \u003c j \\leqslant n}\\left(x_j-x_i\\right) \\end{aligned} $$ 计算降次的Vandermonde行列式时，将上式修改为 $(x_i-x_j)$。\n定义3(行列式的组合定义) 设方阵 $A=\\left(a_{i j}\\right) \\in M_n(\\mathbb{F})$ ，则 $A$ 的行列式定义为\n$$ |A|=\\sum_{\\left(i_1, i_2, \\cdots, i_n\\right) \\in S_n}(-1)^{N\\left(i_1, i_2, \\cdots, i_n\\right)} a_{i_1 1} a_{i_2 2} \\cdots a_{i_n n} $$\n其中 $S_n$ 是 ${1,2, \\cdots, n}$ 的所有全排列构成的集合， $N\\left(i_1, i_2, \\cdots, i_n\\right)$ 是全排列 $\\left(i_1, i_2, \\cdots, i_n\\right)$ 的逆序数 ，$a_{i_1 1} a_{i_2 2} \\cdots a_{i_n n}$ 称为行列式 $|A|$ 中的单项，它从 $A$ 的每行每列各取一个元素相乘得到， $(-1)^{N\\left(i_1, i_2, \\cdots, i_n\\right)}$ 称为这个单项的符号． 容易看出：当 $n \\geq 2$ 时，符号为 $\\pm 1$ 的单项各为一半，即 $\\frac{1}{2} n!$ 个．\n定理4 (Laplace 定理) 设 $|\\boldsymbol{A}|$ 是 $n$ 阶行列式，在 $|\\boldsymbol{A}|$ 中任取 $k$ 行（列），那么含于这 $k$ 行（列）的全部 $k$ 阶子式与它们所对应的代数余子式的乘积之和等于 $|A|$ ．即若取定 $k$ 个行： $1 \\leq i_1\u0026lt;i_2\u0026lt;\\cdots\u0026lt;i_k \\leq n$ ，则\n$$ \\begin{aligned} |\\boldsymbol{A}|=\\sum_{1 \\leq j_1\u0026lt;j_2\u0026lt;\\cdots\u0026lt;j_k \\leq n} \\boldsymbol{A}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k\\\\ j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) \\widehat{\\boldsymbol{A}}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k \\\\ j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) . \\end{aligned} $$\n同样若取定 $k$ 个列： $1 \\leq j_1\u0026lt;j_2\u0026lt;\\cdots\u0026lt;j_k \\leq n$ ，则\n$$ \\begin{aligned} |\\boldsymbol{A}|=\\sum_{1 \\leq i_1\u0026lt;i_2\u0026lt;\\cdots\u0026lt;i_k \\leq n} \\boldsymbol{A}\\left( \\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k \\\\ j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) \\widehat{\\boldsymbol{A}}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k \\\\ j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) . \\end{aligned} $$\n注. Laplace定理可以让我们以任意几行或者几列将行列式进行展开，在理论分析中有重要作用， 可以证明以下推论:\n推论 5 以分块的形式表示行列式，若$|A|$为分块上三角或分块下三角行列式，有\n$$ \\begin{aligned} |A| \u0026amp; =\\left|\\begin{array}{ll} B \u0026amp; C \\\\ 0 \u0026amp; D \\end{array}\\right|=\\left|\\begin{array}{ll} B \u0026amp; 0 \\\\ C \u0026amp; D \\end{array}\\right| =|B||D| \\end{aligned} $$\n第二章 矩阵 矩阵运算 矩阵的加、减、数乘、相乘、转置，共轭，这里重点关注矩阵相乘。\n定义 1(矩阵相乘) 设 $m\\times k$ 矩阵 $A=(a_{ij})$，以及 $k\\times n$ 矩阵 $B=(b_{ij})$，设 $C = AB$，则 $C$ 为 $m\\times n$ 矩阵且： $$ c_{ij} = \\sum_{r=1}^k a_{ir} b_{rj} $$ 注. 介绍矩阵相乘的一些性质和需要注意的点：\n矩阵相乘具有结合律、分配律的性质，但是不具有交换律。 $cI_n$ 被称为纯量阵，有 $AcI_n=cA$. 当 $AB=BA$时，可以使用正常数乘中性质，例如$(AB)^r = A^rB^r$. 矩阵乘法不可交换，所以消去律一般不成立，即若 $AB=AC$,一般不可以推出 $B=C$,同样的，两个非零矩阵相乘可能为零矩阵。 补充一点关于转置的性质：$(AB)^{\u0026rsquo;} = B^{\u0026rsquo;}A^{\u0026rsquo;}$ 可以定义矩阵的行列式为一种从$M_n(\\mathbb{R})$到$\\mathbb{R}$的映射，这种映射本身具有一些性质：\n一般的，$|A|+|B|\\neq|A+B|$ $|cA| = c^n|A|$ $|A||B| = |AB|$ $|A^{\u0026rsquo;}| = |A|$ $|\\bar{A}| = \\bar{|A|}$ $|A|^{-1} = |A^{-1}|$ 设$n$大于等于$2$，则$|A^{*}| = |A|^{n-1}$ 初等变换和初等矩阵 矩阵的行(列)互换，行(列)数乘一个非零常数，行(列)乘常数加到另外一行(列)上为矩阵的三种初等变换。\n定义 1(相抵). 将一个矩阵$A$经过有限次初等变换后变成$B$,则称$A$和$B$是等价的，或者$A$和$B$相抵，记作$A~B$.\n定理 2(相抵标准型). 任意 $m\\times n$矩阵必然相抵于下列 $m\\times n$矩阵 $$ \\begin{aligned} \\left(\\begin{array}{ll} I_r \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{array}\\right) \\end{aligned} $$ 该矩阵被称为原始矩阵的相抵标准型，与如何初等变换无关，这是矩阵自身的性质。\n定理 3. 只使用初等行变换，矩阵最多可以简化为上阶梯型矩阵。\n定理 4(初等矩阵). 对单位阵施以第一类，第二类，第三类初等变换后得到的矩阵分别称为第一类、第二类、第三类初等矩阵，分别记为 $P_{ij},P_i(c),T_{ij}(c)$.\n定理 5. 初等行(列)变换等价于左(右)乘对应的初等矩阵，即\n初等行变换: $P_{ij}A,P_i(c)A,T_{ij}(c)A$ 初等列变换：$AP_{ij},AP_i(c),AT_{ji}(c)$ 一些推论.\n初等阵都是可逆矩阵，且其逆矩阵都为同类型的初等阵，有 $$ P_{ij}^{-1} = P_{ij},P_i(c)^{-1} = P_i(\\frac{1}{c}),T_{ij}(c)^{-1} = T_{ij}(-c) $$ $|P_{ij}| = -1,|P_i(c)| = c,T_{ij}(c) = 1$,对矩阵实施第三类初等变换， 矩阵行列式的值不变。 奇异(非奇异)矩阵经过初等变换后依然是奇异(非奇异)矩阵。 推论 6. 设 $m\\times n$阶矩阵 $A$,存在$m$阶非奇异阵 $P$和 $n$阶非奇异阵 $Q$,可使得 $A$ 变化为相抵标准型，即 $$ PAQ = \\begin{aligned} \\left(\\begin{array}{ll} I_r \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{array}\\right) \\end{aligned} $$\n有了初等变换的关系后，我们现在来判定初等变换符合什么类型的关系。\n定义 6(等价关系). 若集合 $A$ 上的二元关系 $R$ 满足以下条件：\n自反性：$\\forall x \\in A, x R x$ 对称性：$\\forall x, y \\in A, x R y \\Longrightarrow y R x$ 传递性：$\\forall x, y, z \\in A, \\quad(x R y \\wedge y R z) \\Longrightarrow x R z$ 则称 $R$ 是一个定义在 $A$ 上的等价关系。习惯上会把等价关系的符号由 $R$ 改写为 $\\sim$ 。 定理 7. 相抵关系是一种等价关系。\n注. 高等代数中会接触三种关系：相抵、相似和合同关系，都是等价关系。\n应用 (求解逆矩阵). 可以通过构造 $ \\left(\\begin{array}{c:c} A\u0026amp;I_n \\end{array}\\right) $ 然后再通过初等行变换得到 $ \\left(\\begin{array}{c:c} I_n\u0026amp;A^{-1} \\end{array}\\right) $ ,或者构造 $ \\left(\\begin{array}{c} A \\\\ \\hdashline I_n \\\\ \\end{array}\\right) $, 然后通过初等列变换得到 $ \\left(\\begin{array}{c} I_n \\\\ \\hdashline A^{-1} \\\\ \\end{array}\\right) $,总之，使得分块矩阵的变换统一，必须同时进行相同的变换。\n注. 该方法同样可以用来求解线性方程组，假设 $AX = B$,则$X = A^{-1}B$,可以构造 $ \\left(\\begin{array}{c:c} A\u0026amp;B \\end{array}\\right) $ 然后再通过初等行变换得到 $ \\left(\\begin{array}{c:c} I_n\u0026amp;A^{-1}B \\end{array}\\right) $,对于$XA=B$,解$X = BA^{-1}$,则可以通过构造矩阵然后初等列变换求得。\n逆矩阵 定义 1(矩阵的逆). 设$A$为n阶方阵，若存在一个n阶方阵$B$,使得 $$ AB = BA = I_n $$ 则$B$为$A$的逆矩阵，记为$B=A^{-1}$,称有逆矩阵的方阵为非奇异阵或可逆矩阵。\n性质. 假设以下提到的方阵都可逆且同阶数\n$(AB)^{-1} = B^{-1}A^{-1}$ $(A^{\u0026rsquo;})^{-1} = (A^{-1})^{\u0026rsquo;}$ 消去律存在 若A可逆，$AB=0$,则$B$为零矩阵。 定义 2. 设 $\\boldsymbol{A}$ 是 $n$ 阶方阵，$A_{i j}$ 是行列式 $|\\boldsymbol{A}|$ 中第 $(i, j)$ 元素 $a_{i j}$ 的代数余子式，则称下列方阵为 $\\boldsymbol{A}$ 的伴随阵：\n$$ \\left(\\begin{array}{cccc} A_{11} \u0026amp; A_{21} \u0026amp; \\cdots \u0026amp; A_{n 1} \\\\ A_{12} \u0026amp; A_{22} \u0026amp; \\cdots \u0026amp; A_{n 2} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ A_{1 n} \u0026amp; A_{2 n} \u0026amp; \\cdots \u0026amp; A_{n n} \\end{array}\\right) \\quad $$ $A$ 的伴随阵通常记为 $A^*$ ．\n引理 3. 方阵$A$和其伴随阵 $A^{*}$ 有以下关系： $$ AA^{*} = A^{*}A = |A|I_n $$\n定理 4. 若$|A|\\neq 0$,则$A$为非奇异阵，且 $$ A^{-1} = \\frac{1}{|A|}A^{*} $$\n定理 5. 若$A,B$都是n阶矩阵，则 $$ |AB| = |A||B| $$\n一些推论.\n方阵可逆的充分必要条件为行列式不等于0. 可逆方阵的乘积依然为可逆方阵，只要乘积中间有一个奇异阵，最后的矩阵为奇异阵。 若$A$可逆，则$|A^{-1}| = |A|^{-1}$ 若$A,B$都为n阶方阵,$AB = I_n$可以推导出$BA = I_n$,反之亦然。 非奇异矩阵有一些很好的性质，我们将其归结于如下的定理：\n定理 6. 对于$n$阶矩阵$A$,以下几种表述等价：\n$A$ 为非奇异矩阵。 $A$ 的相抵标准型为 $I_n$。 $A$ 可以只通过初等行变换或初等列变换变为 $I_n$。 $A$ 可以分解为若干个初等矩阵的乘积。 分块矩阵 分块矩阵是为了运算和分析更简便自然推广出的表示方法，一般表示为$A = (A_{ij})_{r\\times s}$,$r\\times s$表示分块成$r$分块行和$s$分块列，记$(m_1,m_2,\\cdots,m_r)$为行分块方式，$(n_1,n_2,\\cdots,n_r)$为列分块方式。\n定义 1. 若分块矩阵$A,B$的行分块方式和列分块方式相同，且每个矩阵块相同，则分块矩阵$A,B$相等。\n关于分块矩阵的运算，和普通矩阵的运算方式大致相同，是普通矩阵运算的推广，以下列出需要注意的几点：\n分块矩阵相乘：$AB$，需要保证$A$的列分块方式和$B$的行分块方式相同。 分块矩阵转置除了对换行列元素外，每个矩阵块也应该转置。 例. 设 $\\boldsymbol{A}$ 是一个 $m \\times n$ 矩阵， $\\boldsymbol{B}$ 是一个 $n \\times r$ 矩阵，可对 $\\boldsymbol{B}$ 作列分块，即将 $B$ 的每个列向量分作一块，记为 $\\boldsymbol{\\beta}_j(j=1,2, \\cdots, r)$ ，则 $$ \\boldsymbol{B}=\\left(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2, \\cdots, \\boldsymbol{\\beta}_r\\right) $$ 又将 $A$ 看成是只分成一块的矩阵，则 $A B$ 可按分块矩阵相乘，且 $A B$ 的列分块为 $$ \\boldsymbol{A} \\boldsymbol{B}=\\left(\\boldsymbol{A} \\boldsymbol{\\beta}_1, \\boldsymbol{A} \\boldsymbol{\\beta}_2, \\cdots, \\boldsymbol{A} \\boldsymbol{\\beta}_r\\right) $$ 同样，可对 $\\boldsymbol{A}$ 作行分块，即将 $\\boldsymbol{A}$ 的每个行向量分作一块，记为 $\\boldsymbol{\\alpha}_i(i=1,2, \\cdots, m)$ ，则 $$ A=\\left(\\begin{array}{c} \\boldsymbol{\\alpha}_1 \\\\ \\boldsymbol{\\alpha}_2 \\\\ \\vdots \\\\ \\boldsymbol{\\alpha}_m \\end{array}\\right) $$ 又将 $B$ 看成是只有一块的矩阵，则 $\\boldsymbol{A B}$ 可按分块矩阵相乘，且 $\\boldsymbol{A B}$ 的行分块为 $$ A B=\\left(\\begin{array}{c} \\alpha_1 B \\\\ \\alpha_2 B \\\\ \\vdots \\\\ \\alpha_m B \\end{array}\\right) $$\n推论 2. 若分块对角阵的每一个矩阵块都可逆，则分块对角矩阵可逆。\n定义 3. 对于分块矩阵$A = (A_{ij})_{r\\times s}$，可以定义三类分块初等变换：\n对换$A$的分块行（分块列）。 某个非奇异阵左乘$A$的某一个分块行，或者右乘某一个分块列。 某个矩阵左乘$A$的某一分块行加到另一分块行上，或右乘某一分块列加到另一分块列上。 注. 分块初等变换是初等变换的推广，分块初等变换1可以看作初等变换1的复合，分块初等变换2可以看作初等变换2、3的复合，分块初等变换3可以看作初等变换3的复合，所以分块初等变换也满足初等变换具有的一些性质，也可以对分块单位矩阵进行分块初等变换定义分块初等矩阵，依然用 $P_{ij},P_i(M),T_{ij}(M)$ 表示。\n定理 4. 分块初等矩阵具有以下性质：\n分块初等阵都是可逆阵 $$ P_{ij}^{-1} = P_{ij}^{\u0026rsquo;},P_i(M)^{-1} = P_i(M^{-1}),T_{ij}(M)^{-1} = T_{ij}(-M) $$ $|P_{ij}| = (-1)^{l},l=m_i\\cdot m_j+(m_i+m_j)\\sum_{i\u0026lt;r\u0026lt;j}m_r$\n$|P_i(M)| = |M|,T_{ij}(M) = 1$ 分块初等行(列)变换等价于左(右)乘对应的分块初等矩阵。 分块初等行变换: $P_{ij}A,P_i(M)A,T_{ij}(M)A$ 分块初等列变换：$AP_{ij}^{\u0026rsquo;},AP_i(M),AT_{ji}(M)$ 注. 分块矩阵进行第三类分块初等变换，分块矩阵的行列式不变，我们可以用该方法简化分块行列式的计算。\n定理 5. 若 $\\boldsymbol{A}$ 是 $m$ 阶可逆阵， $\\boldsymbol{D}$ 是 $n$ 阶矩阵， $\\boldsymbol{B}$ 为 $m \\times n$ 矩阵， $\\boldsymbol{C}$ 为 $n \\times m$ 矩阵，则 $$ \\left|\\begin{array}{ll} A \u0026amp; B \\\\ C \u0026amp; D \\end{array}\\right|=|A|\\left|D-C A^{-1} B\\right| $$ 若 $\\boldsymbol{D}$ 可逆（这时 $\\boldsymbol{A}$ 不必假设可逆），则有 $$ \\left|\\begin{array}{ll} A \u0026amp; B \\\\ C \u0026amp; D \\end{array}\\right|=\\left|D | A-B D^{-1} C\\right| $$ 证明 用第三类分块初等变换，以 $-\\boldsymbol{C} \\boldsymbol{A}^{-1}$ 左乘以第一分块行加到第二分块行上得到 $$ \\left(\\begin{array}{ll} A \u0026amp; B \\\\ C \u0026amp; D \\end{array}\\right) \\rightarrow\\left(\\begin{array}{cc} A \u0026amp; B \\\\ O \u0026amp; D-C A^{-1} B \\end{array}\\right) $$ 第三类分块初等变换不改变行列式的值，由引理即得结论．另一结论类似可证．证毕。\n注 1. 当 $\\boldsymbol{A}$ 和 $\\boldsymbol{D}$ 都是可逆阵时，我们得到等式 $$ \\left|D \\right|\\left| A-B D^{-1} C\\right|=|A|\\left|D-C A^{-1} B\\right| $$ 这个等式称为行列式的降阶公式．因为当 $\\boldsymbol{D}$ 和 $\\boldsymbol{A}$ 的阶不等时，可以利用它把高阶行列式的计算化为低阶行列式的计算:如果我们要计算矩阵$M$的行列式，可以构造 $M = A-BD^{-1}C$,然后利用 $|M| = |A-BD^{-1}C| = |D|^{-1}|A|\\left|D-C A^{-1} B\\right|$得到 $M$ 的行列式。\n注 2. 利用分块矩阵的三类初等变换，同样可以求分块矩阵的逆矩阵，或者求解线性方程组。\nCauchy-Binet 公式 定理 1. 设 $\\boldsymbol{A}=\\left(a_{i j}\\right)$ 是 $m \\times n$ 矩阵， $\\boldsymbol{B}=\\left(b_{i j}\\right)$ 是 $n \\times m$ 矩阵，$r$ 是一个正整数且 $r \\leq m$ ． （1）若 $r\u0026gt;n$ ，则 $A B$ 的任意一个 $r$ 阶子式等于零； （2）若 $r \\leq n$ ，则 $\\boldsymbol{A} \\boldsymbol{B}$ 的 $r$ 阶子式 $$ \\begin{aligned} \u0026amp;\\boldsymbol{A} \\boldsymbol{B}\\left(\\begin{array}{llll} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_r \\\\ j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_r \\end{array}\\right)\\\\ \u0026amp; =\\sum_{1 \\leq k_1\u0026lt;k_2\u0026lt;\\cdots\u0026lt;k_r \\leq n} \\boldsymbol{A}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_r \\\\ k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_r \\end{array}\\right) \\boldsymbol{B}\\left(\\begin{array}{cccc} k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_r \\\\ j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_r \\end{array}\\right) \\text {. } \\end{aligned} $$\n注. 若 $r = m$,则$AB$的$r$阶子式就是 $|AB|$,$i,j$的顺序就是$1,2,\\cdots,n$;若 $ r = 1 $,该公式表示矩阵乘法。\n推论 2. 若 $A$ 为 $m\\times n$,则 $AA^{\u0026rsquo;}$的任意主子式都非负。\n注. 若$r$阶子式的行指标和列指标相同，称该子式为主子式。\n推论 3. 若 $AB$ 为 $n$ 阶方阵，则 $(AB)^{*} = B^{*}A^{*} $.\n推论 4. $(AB)^{*} = B^{*}A^{*}$\n对转置，求逆，伴随的一些总结\n$(A^{-1})^{\u0026rsquo;} = (A^{\u0026rsquo;})^{-1},(A^{\u0026rsquo;})^{*} = (A^{*})^{\u0026rsquo;},(A^{-1})^{*} = (A^{*})^{-1}$,三种符号均可以交换顺序 $(AB)^{*} = B^{*}A^{*},(AB)^{-1} = B^{-1}A^{-1},(AB)^{\u0026rsquo;} = B^{\u0026rsquo;}A^{\u0026rsquo;}$，拆开括号后顺序相反各自运算 $(A^{\u0026rsquo;})^{\u0026rsquo;} = A,(A^{-1})^{-1} = A$ $ (A^{*})^{*} = \\left\\{ \\begin{array}{lc} |A|^{n-2}A \u0026amp;, n \\geq3 \\\\ A\u0026amp;,n=2\\\\ \\end{array} \\right.$,二次运算后不完全相同 $(cA)^{\u0026rsquo;} = cA^{\u0026rsquo;},(cA)^{-1} = c^{-1}A^{-1},(cA)^{*} = c^{n-1}A^{*}$ $AA^{*} = A^{*}A = |A|I_n,A^{-1}A = A^{-1}A = I_n,AA^{\u0026rsquo;}$半正定 $|A^{*}| = |A|^{n-1},|A^{\u0026rsquo;}| = |A|,|A^{-1}| = |A|^{-1}$ 补充. 相抵标准型有一些很好的性质，易于求伴随就是其中一项，令$A = P\\begin{aligned} \\left(\\begin{array}{ll} I_r \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{array}\\right) \\end{aligned}Q$是一种分析和证明的方法。\n第三章 线性空间 数域、线性空间以及线性关系 定义 1(数域，数环). 设 $\\mathbb{K}$ 是复数集 $\\mathbb{C}$ 的子集，且至少有两个不同的元素(等价定义：至少含有1和0)，如果 $\\mathbb{K}$ 中任意两个数的加减乘除运算依然属于 $\\mathbb{K}$，则 $\\mathbb{K}$ 被称为一个数域，如果 $\\mathbb{K}$ 对加减乘三个运算封闭，则 $\\mathbb{K}$ 被称为一个数环，所以数域也是数环。\n注. $\\mathbb{Z\\subseteq Q\\subseteq R\\subseteq C} $,即整数属于有理数属于实数属于复数，有理数、实数、复数都是数域，整数是数环。\n定理 2. 任何一个数域都包含有理数域 $\\mathbb{Q}$,即有理数域是最小的数域。\n定义 3(线性空间). 设 $\\mathbb{K}$ 是一个数域，$V$ 是一个集合.在 $V$ 上定义了一个加法＂+ ＂： $V\\times V\\rightarrow V$, 即对 $V$ 中任意两个元素 $\\alpha, \\beta$ ，总存在 $V$ 中唯一的元素 $\\gamma$ 与之对应，记为 $\\gamma=$ $\\alpha+\\beta$ ．在数域 $\\mathbb{K}$ 与 $V$ 之间定义了一种运算，称为数乘\u0026quot;$\\cdot$\u0026quot;: $\\mathbb{K} \\times V \\rightarrow V $ ，即对 $\\mathbb{K}$ 中任一数 $k$ 及 $V$中任一元素 $\\boldsymbol{\\alpha}$ ，在 $V$ 中总有唯一的元素 $\\delta$ 与之对应，记为 $\\delta=k \\boldsymbol{\\alpha}$ ．若上述加法及数乘满足下列运算规则：\n（1）加法交换律：$\\alpha+\\beta=\\beta+\\alpha$ ； （2）加法结合律：$(\\boldsymbol{\\alpha}+\\boldsymbol{\\beta})+\\boldsymbol{\\gamma}=\\boldsymbol{\\alpha}+(\\boldsymbol{\\beta}+\\boldsymbol{\\gamma})$ ；\n（3）在 $V$ 中存在一个元素 0 ，对于 $V$ 中任一元素 $\\alpha$ ，都有 $\\alpha+0=\\alpha$ ；\n（4）对于 $V$ 中每个元素 $\\alpha$ ，存在元素 $\\boldsymbol{\\beta}$ ，使 $\\alpha+\\beta=0$ ；\n（5） $1 \\cdot \\alpha=\\alpha$ ；\n（6）$k(\\boldsymbol{\\alpha}+\\boldsymbol{\\beta})=k \\boldsymbol{\\alpha}+k \\boldsymbol{\\beta}$ ；\n（7）$(k+l) \\boldsymbol{\\alpha}=k \\boldsymbol{\\alpha}+l \\boldsymbol{\\alpha}$ ；\n（8）$k(l \\boldsymbol{\\alpha})=(k l) \\boldsymbol{\\alpha}$ ，\n其中 $\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}$ 是 $V$ 中任意的元素，$k, l$ 是 $\\mathbb{K}$ 中任意的数，则集合 $V$ 称为数域 $\\mathbb{K}$ 上的线性空间或向量空间，$\\mathbb{K}$被称为线性空间$V$的基域。$V$ 中的元素称为向量 $V$ 中适合（3）的元素 0 称为零向量．对 $V$ 中的元素 $\\alpha$ ，适合 $\\alpha+\\beta=0$ 的元素 $\\beta$ 称为 $\\alpha$ 的负向量，记为 $-\\alpha$ ．\n注 1. 在实际问题中，我们经常会碰到满足上述八条性质的集合，例如信号处理中使用的信号空间，并且有必要将其抽象成线性空间，来研究其统一性质，对于抽象的线性空间，向量只是该集合的一个元素，没有任何其它附加的含义。\n注 2. 数域 $\\mathbb{K}$ 上的 $n$ 维行向量（列向量）集合被称为 $\\mathbb{K}$ 上的线性空间，经常将其记为 $\\mathbb{K}_n(\\mathbb{K}^n)$。\n命题 4. 现在说明一些线性空间的基本性质：\n零向量唯一。 负向量唯一。 加法满足消去律：等式两边可以同时减去一个数并且保证等式成立。 $-\\alpha = (-1)\\alpha$ 若 $k\\alpha = 0$,则 $k = 0$ 或 $\\alpha = 0$。 问题. 考虑线性方程组问题：$Ax = \\beta$,$A\\in M_{m\\times n}(\\mathbb{K})$。\n可以将 $A$ 写为列分块的方式进行计算： $$ Ax =\\left(\\alpha_1,\\alpha_2,\\cdots,\\alpha_n \\right)\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{array}\\right) = \\sum_{i = 1}^{n} x_i\\alpha_i = \\beta $$\n定义 5. $V$ 是 $\\mathbb{K}$ 上的线性空间(以后简记为$V_\\mathbb{K}$)，向量 $\\alpha_1,\\alpha_2,\\cdots,\\alpha_n,\\beta\\in V$,若存在 $\\mathbb{K}$ 中的 $n$ 个数 $k_1,k_2\\cdots k_n$,s.t.\n$$ \\sum_{i = 1}^{n} k_i\\alpha_i = \\beta $$ 则称 $\\beta$ 为 $\\alpha_1,\\alpha_2,\\cdots,\\alpha_n$ 的线性组合或者 $\\beta$可以由 $\\alpha_1,\\alpha_2,\\cdots,\\alpha_n$ 线性表示。\n定义 6. 设 $V$ 是数域 $\\mathbb{K}$ 上的线性空间， $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2, \\cdots, \\boldsymbol{\\alpha}_n$ 是 $V$ 中的 $n$ 个向量，若存在 $\\mathbb{K}$ 中不全为零的 $n$ 个数 $k_1, k_2, \\cdots, k_n$ ，使 $$ k_1 \\boldsymbol{\\alpha}_1+k_2 \\boldsymbol{\\alpha}_2+\\cdots+k_n \\boldsymbol{\\alpha}_n=\\mathbf{0} $$ 则称 $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2, \\cdots, \\boldsymbol{\\alpha}_n$ 线性相关．反之，若不存在 $\\mathbb{K}$ 中不全为零的数 $k_1, k_2, \\cdots, k_n$ ，使上式成立，则称 $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2, \\cdots, \\boldsymbol{\\alpha}_n$ 线性无关或线性独立．\n注. 线性相关性的存在与否极大程度上依赖于线性空间的基域，如果基域发生变化。线性相关性可能会发生变化，线性无关也有等价定义：使其线性组合为0的系数全为0.\n定理 7. 若 $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2, \\cdots, \\boldsymbol{\\alpha}_m$ 是一组线性相关的向量，则任一包含这组向量的向量组必线性相关，又若 $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2, \\cdots, \\boldsymbol{\\alpha}_m$ 是一组线性无关的向量，则从这一组向量中任意取出一组向量必线性无关。\n定理 8. 设 $\\alpha_1, \\alpha_2, \\cdots, \\alpha_m$ 是线性空间 $V$ 中的向量，则 $\\alpha_1, \\alpha_2, \\cdots, \\alpha_m$线性相关的充分必要条件是其中至少有一个向量可以表示为其余向量的线性组合．\n定理 9（线性表示唯一的条件）. 设 $\\alpha_1, \\alpha_2, \\cdots, \\alpha_m, \\beta$ 是线性空间 $V$ 中的向量．已知 $\\beta$ 可表示为 $\\alpha_1, \\alpha_2, \\cdots, \\alpha_m$ 的线性组合，即 $$ \\boldsymbol{\\beta}=k_1 \\boldsymbol{\\alpha}_1+k_2 \\boldsymbol{\\alpha}_2+\\cdots+k_m \\boldsymbol{\\alpha}_m $$ 则表示唯一的充分必要条件是向量 $\\boldsymbol{\\alpha}_1, \\boldsymbol{\\alpha}_2, \\cdots, \\boldsymbol{\\alpha}_m$ 线性无关．\n线性相关的几何意义及思考\n对于二维欧氏空间 $\\mathbb{R}^2$,存在向量 $OA = (a_1,a_2),OB = (b_1,b_2)$,若二者线性相关，则 $OAB$共线;若线性无关，则 $OAB$ 构成一个非退化的三角形，面积为$\\frac{1}{2}\\left|\\begin{array}{ll} a_1 \u0026amp; a_2 \\\\ b_1 \u0026amp; b_2 \\end{array}\\right|$,该行列式不为0.\n对于三维欧氏空间 $\\mathbb{R}^3$,存在向量 $OA = (a_1,a_2,a_3),OB = (b_1,b_2,b_3),OC = (c_1,c_2,c_2)$,若三者线性相关，则 $OABC$四点共面;若线性无关，则 $OABC$ 构成一个非退化的四面体，体积为$\\frac{1}{6}\\left|\\begin{array}{lll} a_1 \u0026amp; a_2 \u0026amp;a_3\\\\ b_1 \u0026amp; b_2 \u0026amp;b_3\\\\ c_1\u0026amp;c_2\u0026amp;c_3 \\end{array}\\right|$,该行列式不为0.\n$\\cdots$\n以此类推，$n$ 维欧氏空间的向量组线性无关的条件是否为行列式不等零？\n第四章 线性映射 第五章 多项式 第六章 特征值 第七章 相似标准型 第八章 二次型 第九章 内积空间 第十章 双线性型 ","date":"2025-01-26T00:21:05+08:00","image":"http://localhost:1313/post_file/%E9%AB%98%E7%AD%89%E4%BB%A3%E6%95%B0/cover.jpg","permalink":"http://localhost:1313/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E9%AB%98%E7%AD%89%E4%BB%A3%E6%95%B0/","title":"学习笔记|高等代数"},{"content":"Course notes 这篇blog用于记录和整理北京邮电大学\u0026amp;QMUL合办专业——电信工程及管理(Telecommunications engineering and management)专业学习笔记，以及一些课程教材推荐。\n大一 高等数学\n参考书目：1) Advanced Mathematics，By Jianhua Yuan,et al\n2) Calculus, 13th Edition,by George B. Thomas, et al\n线性代数\n参考书目：1) linear algebra，by Wenbo Zhang,et al\n2) Linear Algebra and its Applications,David C.Lay,et al\n大学物理\n课程笔记：note\n参考书目：1) Physics for Scientists and Engineers with Modern Physics: D. C. Giancoli.\n2) Berkeley Physics Course\n3) 电磁学（第三版）,作者：贾起民、郑永令、陈暨耀\n4) 新概念物理学教程 光学，作者：赵凯华\n电路分析\n参考书目：电路分析基础（第二版）（英文版）作者：王宏祥；James W. Nilsson, Susan A. Riedel\nC语言程序设计基础\n大二 数字电路设计\n课程笔记:note 参考书目:Digital Design: Principles and Practices, 5th edition by John F. Wakerly\nJAVA\n课程笔记：note 参考书目：Head First Java, 3rd Edition by Kathy Sierra, Bert Bates, Trisha Gee\n概率论与随机过程\n课程笔记：note\n参考书目：概率论与随机过程,作者:周清 张丽华\n参考课程：随机过程 张颢 2024年春\n计算机网络\n课程笔记：note\n参考书目：Computer Networking: A Top-Down Approach,Authors:PictureJames F. Kurose,PictureKeith W. Ross\n数字信号处理\n课程笔记：note\n参考书目：Discrete Time Signal Processing by Alan V. Oppenheim and Ronald W. Schafer.\n参考课程：数字信号处理 张颢 2023年秋\n工程数学\n课程笔记：note\n参考书目：Engineering Mathematics，By Xia Shi\n信号与系统\n课程笔记：note\n参考书目：Signals and Systems 2nd Edition，by Alan Oppenheim , Alan Willsky , S. Nawab\nAI导论\n课程笔记：note\n强化学习基础：note\n参考课程：1) 强化学习基础 （本科生课程） 北京邮电大学 鲁鹏\n2) 机器视觉教学课程\n电子电路基础\n课程笔记：note\n参考书目：1) 新概念模拟电路，作者：杨建国\n2) 模拟电子技术基础（第五版）,作者：童诗白等\n数值计算方法\n参考书目：沈剑华,数值计算基础（第二版）\n数据结构\n课程笔记：note\n参考书目：数据结构，作者：严蔚敏\n企业管理\n课程笔记：note\n参考视频：【元首的愤怒】北邮国院大二企业管理期末考试\n大三 通信原理\n课程笔记：note\n参考书目： 1) 通信原理（第4版） 周炯磐 庞沁华 续大我 吴伟陵 杨鸿文\n2) Fundamentals of Communication Systems.John G.Proakis;Masoud Salehi\n电磁场与电磁波\n课程笔记：note\n参考书目：Electromagnetic Field Theory Fundamentals,Guru\n机器学习\n课程笔记：note\n参考书目：1) Foundations of Machine Learning,second edition,Mehryar Mohri, Afshin Rostamizadeh,and Ameet Talwalkar\n2) Understanding Machine Learning:From Theory to Algorithms,by Cambridge University Press.\n参考课程：Introduction to Machine Learning\n网络编程\n课程笔记：note\n参考书目：An Introduction to Network Programming with Java, by Jan Graba, 3rd Edition\n数字系统设计\n课程笔记：note\n参考书目：Circuit design with VHDL by Pedroni, Volnei A\n微波工程\n参考书目：Microwave Engineering,by David M. Pozar.\n微处理器系统设计\n参考书目：课堂讲义\n软件工程\n参考书目：课堂讲义\n产品开发与营销\n参考书目：课堂讲义 ","date":"2025-01-11T00:21:05+08:00","image":"http://localhost:1313/post_file/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/cover.png","permalink":"http://localhost:1313/p/course-notes/","title":"Course notes"},{"content":"Lab\u0026rsquo;s group meeting on December 2.The article Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions was shared at the group meeting, see the pdf of this slide:Slide.\n","date":"2024-12-02T16:50:00+08:00","image":"http://localhost:1313/post_file/sampling_theory/cover.png","permalink":"http://localhost:1313/p/sampling-is-as-easy-as-learning-the-score-theory-for-diffusion-models-with-minimal-data-assumptions/","title":"Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions"},{"content":"Score-Based Variational Inference for Inverse Problems Lab\u0026rsquo;s group meeting on September 1.The article Score-Based Variational Inference for Inverse Problems was shared at the group meeting, see the pdf of this tutorial:Score-Based Variational Inference for Inverse Problems.\n","date":"2024-09-19T20:00:00+08:00","image":"http://localhost:1313/post_file/Score-Based_Variational_Inference_for_Inverse_Problems/cover.png","permalink":"http://localhost:1313/p/score-based-variational-inference-for-inverse-problems/","title":"Score-Based Variational Inference for Inverse Problems"},{"content":"My mathematical modeling 这篇blog用于记录和整理我参加过的数学建模比赛以及论文。\n2024年美国大学生数学竞赛(D题)，五大湖水位控制：PDF。 2024年全国大学生统计建模竞赛，山火预测与调控：PDF。 2024年北京邮电大学数学建模训练营选拔，定日镜场优化：PDF。 2024年全国大学生数学竞赛(B题)，生产过程中的决策问题:PDF。 ","date":"2024-08-17T00:21:05+08:00","image":"http://localhost:1313/post_file/My_mathematical_modeling/cover.png","permalink":"http://localhost:1313/p/my-mathematical-modeling/","title":"My mathematical modeling"},{"content":"Bayesian signal processing 该博客为张颢老师所讲的现代信号处理的贝叶斯信号处理部分的课程个人学习笔记,涵盖贝叶斯公式的含义,贝叶斯学派与频率学派的关系,贝叶斯统计模型的目标函数,蒙特卡洛采样和贝叶斯滤波的内容。\nIntroduce to bayes method Bayes and frequency 使用贝叶斯公式并不意味着在使用贝叶斯方法,频率学派也会经常使用贝叶斯公式,早在贝叶斯之前,贝叶斯公式就已经在Laplce等数学家中非常娴熟的应用,贝叶斯方法在思想上面的含义要远远大于数学形式上的贝叶斯公式。\n摘抄知乎上Xiangyu Wang的回答：频率学派和贝叶斯学派最大的差别其实产生于对参数空间的认知上。所谓参数空间,就是你关心的那个参数可能的取值范围。频率学派（其实就是当年的Fisher）并不关心参数空间的所有细节,他们相信数据都是在这个空间里的“某个”参数值下产生的（虽然你不知道那个值是啥）,所以他们的方法论一开始就是从“哪个值最有可能是真实值”这个角度出发的。于是就有了最大似然（maximum likelihood）以及置信区间（confidence interval）这样的东西,你从名字就可以看出来他们关心的就是我有多大把握去圈出那个唯一的真实参数。而贝叶斯学派恰恰相反,他们关心参数空间里的每一个值,因为他们觉得我们又没有上帝视角,怎么可能知道哪个值是真的呢？所以参数空间里的每个值都有可能是真实模型使用的值,区别只是概率不同而已。于是他们才会引入先验分布（prior distribution）和后验分布（posterior distribution）这样的概念来设法找出参数空间上的每个值的概率。最好诠释这种差别的例子就是想象如果你的后验分布是双峰的,频率学派的方法会去选这两个峰当中较高的那一个对应的值作为他们的最好猜测,而贝叶斯学派则会同时报告这两个值,并给出对应的概率。\n假如我们有一个统计模型$f(x,\\theta)$去估计原始数据$x$的分布,频率学派认为$\\theta$是固定的,尽管我们现在还没有办法得到,贝叶斯学派认为$\\theta$是随机的,是服从一个概率分布的随机变量。\nBayes: The Science of cognitive law 贝叶斯的观点是符合人脑的认知规律的,也是现代认知心理学的基础：在认识一件事物之前,我们会先对这个事物有一定的认知,在与事物接触的过程中,我们的认知会逐渐的发生变化,或加强我们的认知,或减弱我们的认知,我们的认知从而得到了进步,这与马原中所说的认知随着实践螺旋式上升也有异曲同工之妙。贝叶斯公式就是表述了这个认知的过程： $$ \\begin{equation} P(\\theta \\mid X)=\\frac{P(X \\mid \\theta) P(\\theta)}{P(X)} \\end{equation} $$ 贝叶斯学派通过引入三个名词对贝叶斯公式进行解释：\n名词 公式 先验分布(Prior) $P(\\theta)$ 似然(Likelihood) $P(X \\mid \\theta)$ 后验分布(Postertor) $P(\\theta \\mid X)$ 先验分布就是在接触事物数据之前我们已经有的认知,似然是在接触事物的过程中,在给定先验知识的条件下我们对数据的认知,后验分布是我们在数据的基础上对事物的认知,机器学习中的“Learning”这一概念,很大程度上就是贝叶斯方法中的使用后验分布去调整先验分布,当数据量足够大的时候,$P(\\theta)$就越来越接近真实的概率模型,另外,分母$P(X)$经常被成为贝叶斯因子。\n我们再来观察特殊的情况。如果数据与先验独立,即$X \\perp \\theta$,有$P(\\theta \\mid X)=P(\\theta)$,即先验等于后验,也就是说在接触数据的过程中,我们并没有学到有用的信息。如果$P(\\theta)=0$,我们的先验分布和后验分布会一直等于0,所以在很多机器学习算法中设置初始迭代解的时候,会避开零概率的设置。\nExample: Naive Bayes\n朴素贝叶斯是一种简单的机器学习分类器,分类任务就是在给定某些特征 $\\lbrace F_i \\rbrace_{i=1}^N$(Features)的条件下,判断某一事物的分类$C$(Catogories),即我们需要获得：$P(C \\mid F_1:F_N)$,数据是给定条件,分类是随机变量,这就是贝叶斯方法中所说的后验分布,我们可以使用先验分布和似然来表示该后验分布： $$ \\begin{equation} P(C \\mid F_1:F_N)=\\frac{P(F_1:F_N \\mid C) P(C)}{P(F_1:F_N)} \\end{equation} $$ 对不同特征求给定条件的联合分布是比较困难的,naive bayes做了一个假设,在给定分类条件下,不同特征之间相互独立,即我们可以重写上述公式为 $$ \\begin{equation} P(C \\mid F_1:F_N)=\\frac{\\prod_{i=1}^N P(F_i \\mid C) P(C)}{P(F_1:F_N)} \\end{equation} $$ 分母可以通过统计样本得出,但是并不重要,因为所有的后验分布共享一个分母,当分类器判别样本时,都具有相同的权重。\nExample:Use bayes to process Gaussian distribution\n联想DDPM中求$q(x_{t-1} \\mid x_t,x_0)$的过程,我们使用贝叶斯公式去计算似然和先验都为Gausssian分布的情况,即$\\lbrace X_1,\u0026hellip;,X_N \\rbrace \\overset{\\mathrm{iid}}{\\sim}N(\\mu,\\sigma_x^2)$,$\\mu\\sim N(\\mu_\\theta,\\sigma_\\mu^2)$,已知先验分布和似然,去求后验分布,即\n$$ \\begin{equation} \\begin{aligned} \u0026 P\\left(\\mu \\mid x_1: x_N\\right)=\\frac{P\\left(x_1: x_N \\mid \\mu\\right) P(\\mu)}{P\\left(x_1: x_N\\right)} \\\\ \u0026 \\propto \\frac{\\exp \\left\\{-\\frac{1}{2 \\sigma_k^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2\\right\\} \\exp \\left\\{-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta\\right)^2\\right\\}}{\\int_{-\\infty}^{+\\infty} \\exp \\left\\{-\\frac{1}{2 \\sigma_x^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta \\right)^2\\right\\} d \\mu} \\end{aligned} \\end{equation} $$ 拿出分母中指数内的部分(其实也是分子部分)：\n$$ \\begin{equation} \\begin{aligned} \u0026 -\\frac{1}{2 \\sigma_x^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta \\right)^2 \\\\ = \u0026 -\\frac{1}{2 \\sigma_x^2}\\left(\\sum_{k=1}^N x_k^2+N \\mu_x^2- \\mu_x \\sum_{k=1}^N x_k\\right)-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu^2+\\mu_\\theta^2-2 \\mu_\\theta \\mu\\right) \\\\ = \u0026 \\left(-\\frac{1}{2 \\sigma_x^2}-\\frac{1}{2 \\sigma_\\mu^2}\\right) \\mu^2+\\left(-\\frac{-2 \\sum_{k=1}^N x_k}{2 \\sigma_x^2}-\\frac{-2 \\mu_\\theta}{2 \\sigma_\\mu^2}\\right) \\mu+Q\\left(x_1:x_N\\right) \\\\ = \u0026 -\\frac{1}{2}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}\\right)\\left(\\mu-\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\right)^2+Q^{\\prime}\\left(x_1:x_N\\right) \\end{aligned} \\end{equation} $$ 余项$Q(x_1:x_N)$在分子与分母中都有,且与随机变量$\\mu$无关,所以原式可以写做正比于分母也是Gaussian分布的形式： $$ \\begin{equation} \\begin{aligned} \u0026P\\left(\\mu \\mid x_1: x_N\\right)\\\\ \u0026\\propto \\frac{\\exp \\left\\{-\\frac{1}{2 \\sigma_k^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2\\right\\} \\exp \\left\\{-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta\\right)^2\\right\\}}{\\int_{-\\infty}^{+\\infty} \\exp \\left\\{-\\frac{1}{2 \\sigma_x^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta \\right)^2\\right\\} d \\mu} \\\\ \u0026\\propto \\frac{\\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}\\right)\\left(\\mu-\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\right)\\mathbf{/}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2} \\right)^2\\right\\}}{\\int_{-\\infty}^{+\\infty}\\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}\\right)\\left(\\mu-\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\mathbf{/}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2} \\right)\\right)^2\\right\\}d \\mu} \\end{aligned} \\end{equation} $$ 分母在积分过后与$\\mu$无关,可以看作一个加权,综合所有的加权,只需注意分子中Gaussian分布的形式即可,那么我们可以得到： $$ P\\left(\\mu \\mid x_1: x_N\\right)\\sim N\\left(\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\mathbf{/}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2} \\right),\\frac{\\sigma_x^2\\sigma_\\mu^2}{N\\sigma_\\mu^2+\\sigma_x^2}\\right) $$ 令$\\bar{X} = \\frac{1}{N}\\sum_{k=1}^N x_k$,可以将后验分布的均值写为 $$ \\mu =\\left(\\frac{\\frac{n}{\\sigma _x^2}}{\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}}\\right) \\bar{X}+\\left(\\frac{\\frac{1}{\\sigma_\\mu^2}}{\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}}\\right) \\mu_\\theta $$ 可见,后验分布可以当作某种似然与先验分布的线性凸组合,当$n$足够大时,数据的加权十分大,就可以忘记先验,当$\\sigma_x$足够大时,数据的信任度很低,学习的权重会尽可能的减少,从而保持先验,其实这种似然和先验分布都是Guassian分布的情况下就是简化的线性高斯模型,可以有更统一的矩阵描述形式。 Statistical decision theory 贝叶斯方法的核心观点就是利用后验分布去学习统计模型$\\hat{\\theta}(x)$,去对先验分布进行优化,那么就需要我们定义一种度量$d(\\hat{\\theta}(x),\\theta)$,从而通过最小化这种度量的方式完成先验分布的优化,$d(\\hat{\\theta}(x),\\theta)$是随机的,所以我们希望消除他的随机性求平均距离,即我们的目标为\n$$ \\begin{equation} \\hat{\\theta}(x) = \\underset{\\hat{\\theta}}{\\arg \\min }\\mathbb{E}_{x,\\theta}[d(\\hat{\\theta}(x),\\theta) ] \\end{equation} $$ 平均距离$\\mathbb{E}_{x,\\theta}[d(\\hat{\\theta}(x),\\theta) ]$ 一般被成为风险(Risk),该目标函数也被称为风险极小化,频率学派和贝叶斯学派对该目标函数有不同方式的处理：\n$$ \\begin{equation} \\begin{aligned} E[d(\\hat{\\theta}(x), \\theta)]\u0026=\\iint d(\\theta, \\hat{\\theta}(x)) P_{x, \\theta}\\left(x, \\theta\\right) d x d\\theta\\\\ \u0026=\\iint d(\\theta, \\hat{\\theta}(x)) P_{x\\mid \\theta}\\left(x\\mid \\theta\\right) d xP(\\theta) d\\theta\\\\ \u0026=\\iint d(\\theta, \\hat{\\theta}(x)) P_{x\\mid \\theta}\\left(\\theta \\mid x\\right) d \\theta P(x) dx\\\\ \\end{aligned} \\end{equation} $$ 可见,频率学派尽可能的削弱$\\theta$的随机性,因为他们认为$\\theta$是确定的（所以在频率学派中有极大化似然这种方法）,但是频率学派种所需要的似然往往是不可以直接计算的,因此有了经验风险极小化(Empirical Risk Minimization,ERM)理论,该理论一般将样本均值取代理论均值,即 $$ E[d(\\hat{\\theta}(x), \\theta)]=\\frac{1}{N}\\sum_{k=1}^Nd(\\theta_k, \\hat{\\theta}_k(x)) $$ 贝叶斯学派则是尽可能削弱数据的随机性,认为已经发生的实验就是确定的,将问题的重心转化到后验分布的计算上,在后面我们会看到,使用贝叶斯方法处理统计模型的关键就是得到后验分布以及如何处理后验分布。\nChoice of measurement MSE\n选择度量为均方误差,可以定义风险(忽略了数据的随机性)： $$ \\begin{equation} R(\\theta, \\hat{\\theta})=\\int(\\theta-\\hat{\\theta})^2 P(\\theta \\mid x) d \\theta \\end{equation} $$ 由拉格朗日乘子法,最优的$\\hat{\\theta}$在风险函数的导数为零处,即使得$\\nabla_{\\hat{\\theta}} R(\\theta, \\hat{\\theta})=0$,我们求解该式,可以得到 $$ \\begin{equation} \\hat{\\theta}^*=\\frac{\\int \\theta p(\\theta \\mid x) d \\theta}{\\int p(\\theta \\mid x) d \\theta}=\\int \\theta p(\\theta \\mid x) d \\theta=E[\\theta \\mid x] \\end{equation} $$ 这与频率学派下的MMSE最优解不谋而合,都是均方误差的形式,但是各自的随机变量不同,由此可见,条件期望就是均方误差意义下的最优估计。\nMAE\n平均绝对误差意义下,风险函数为 $$ \\begin{equation} R(\\theta, \\hat{\\theta})=\\int\\mid \\theta-\\hat{\\theta}\\mid P(\\theta \\mid x) d \\theta \\end{equation} $$ 由于风险函数存在绝对值,所以无法直接求导,我们可以将绝对值拆开写为 $$ \\begin{equation} R(\\theta, \\hat{\\theta})=-\\int_{-\\infin}^{\\hat{\\theta}}( \\theta-\\hat{\\theta}) P(\\theta \\mid x) d \\theta+\\int_{\\hat{\\theta}}^{+\\infin}(\\theta-\\hat{\\theta}) P(\\theta \\mid x) d \\theta \\end{equation} $$ 现在我们就可以利用变上下限积分的技巧对风险函数进行求导,求导的过程就是把$\\theta-\\hat{\\theta}$拆开,使得积分的变上限不在积分变量之内,在利用求导法则正常求导即可,可以求得： $$ \\nabla_{\\hat{\\theta}} R(\\theta, \\hat{\\theta})=\\int_{-\\infty}^{\\hat{\\theta}} p(\\theta \\mid x) d \\theta-\\int_{\\hat{\\theta}}^{+\\infty} p(\\theta \\mid x) d \\theta=0 $$ 直观的观察这个式子,当风险函数的导数等于0时,$\\hat{\\theta}$就是平分后验分布概率密度函数面积的垂直于$\\theta$轴的直线,也就是统计量中的中位数,即中位数是MAE意义下的最优估计。\nMAP\n这里的MAP指极大化后验概率的意思,定义度量为：\n$$ \\begin{equation} d(\\theta, \\hat{\\theta})=\\left\\{\\begin{array}{l} 1,|\\hat{\\theta}-\\theta|\u003e\\delta \\\\ 0,|\\hat{\\theta}-\\theta| \\leq \\delta \\end{array}\\right. \\end{equation} $$ 其中,$\\delta$是我们主观挑选的误差容忍数值,当$\\delta$足够小时,风险函数可以写为：\n$$ \\begin{equation} \\begin{aligned} R(\\theta, \\hat{\\theta}) \u0026 =\\int d(\\theta, \\hat{\\theta}) p(\\theta, x) d \\theta \\\\ \u0026 =\\int_{|\\hat{\\theta}-\\theta|\u003e\\delta} p(\\theta \\mid x) d \\theta \\\\ \u0026 =1-\\int_{|\\hat{\\theta}-\\theta| \\leqslant \\delta} p(\\theta \\mid x) d \\theta \\\\ \u0026 \\approx 1-2 \\delta p(\\theta \\mid x) \\end{aligned} \\end{equation} $$ 那么,我们就可以等价的将最小化风险写为最大化后验概率,即 $$ \\begin{equation} \\underset{\\hat{\\theta}}{\\arg \\min }R(\\theta,\\hat{\\theta})\\iff\\underset{\\hat{\\theta}}{\\arg \\max }p(\\theta \\mid x) \\end{equation} $$ 用统计量来表述的话就是最大化众数。 Example:Use bayes to process Bernoulli distribution\n假设我们有先验分布$\\theta$,似然$X\\sim B(n,\\theta)$,目标计算$\\theta$的后验分布以及在均方意义下的最优估计。\n由Bayes公式,有\n$$ \\begin{equation} \\begin{aligned} P(\\theta \\mid x) \u0026 =\\frac{P(x \\mid \\theta) P(\\theta)}{P(x)} \\\\ \u0026 =\\frac{P(x \\mid \\theta) P(\\theta)}{\\int P(x, \\theta) d \\theta} \\end{aligned} \\end{equation} $$ 接下来,我们考虑当先验分布服从标准均匀分布的情况下后验概率的计算,即计算 $$ \\begin{equation} P(\\theta \\mid x)=\\frac{\\theta^x(1-\\theta)^{n-x}}{\\int_0^1 \\theta^x(1-\\theta)^{n-x} d \\theta} \\end{equation} $$ 在计算后验概率之前,我们需要引入Beta分布来辅助我们的贝叶斯因子的计算,Beta分布的pdf为\n$$ \\begin{equation} \\begin{aligned} f(x ; \\alpha, \\beta) \u0026 =\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int_0^1 u^{\\alpha-1}(1-u)^{\\beta-1} d u} \\\\ \u0026 =\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} \\\\ \u0026 =\\frac{1}{\\mathrm{~B}(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} \\end{aligned} \\end{equation} $$ 其中,$\\alpha,\\beta\u0026gt;0$,$\\Gamma(x)$是Gamma函数,有性质 $$ \\Gamma(x)=\\int_0^{+\\infty} t^{x-1} e^{-t} d t(x\u0026gt;0)=(x-1)! $$ Beta函数$B(\\alpha, \\beta)$的定义为$B(\\alpha, \\beta)=\\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1} \\mathbf{d} t$,现在我们来证明$\\mathrm{~B}(\\alpha, \\beta)=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$:\n观察Beta函数的定义,类似卷积的定义,注意到：\n$$ \\begin{aligned} B(\\alpha, \\beta)\u0026=\\int_0^1 t^{\\alpha-1}(x-t)^{\\beta-1} \\mathbf{d} t\\mid_{x=1} \\\\\u0026=f(x)*g(x)\\mid_{x=1} \\end{aligned} $$ 在上述公式中,我们定义了两个幂函数$f(x): =x^{\\alpha-1},g(x):=x^{\\beta-1},$这两个幂函数的支撑集应该是[0,1],这样才能满足积分的上下限要求,现在对两个幂级数的卷积进行Laplce变换：\n$$ \\mathcal{L}\\left\\{x^{\\alpha-1} * x^{\\beta-1}\\right\\}=\\mathcal{L}\\left\\{x^{\\alpha-1}\\right\\} \\cdot \\mathcal{L}\\left\\{x^{\\beta-1}\\right\\} $$ 而幂级数的Laplace变换可以和Gamma函数联系起来,即\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}\\left\\{t^a\\right\\}\u0026=\\int_0^{\\infty} t^a e^{-s t} d t \\\\ \u0026=\\frac{1}{s^{a+1}} \\int_0^{\\infty}(s t)^a e^{-(s t)}(s d t) \\\\ \u0026=\\frac{1}{s^{a+1}} \\int_0^{\\infty} \\tau^{(a+1)-1} e^{-\\tau} d \\tau \\\\ \u0026=\\frac{\\Gamma(a+1)}{s^{a+1}} \\end{aligned} \\end{equation}$$ 继续我们上述的Laplace变换,有\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}\\left\\{x^{\\alpha-1}\\right\\} \\cdot \\mathcal{L}\\left\\{x^{\\beta-1}\\right\\}\u0026=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{s^{\\alpha+\\beta}} \\\\ \u0026=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\frac{\\Gamma(\\alpha+\\beta-1+1)}{s^{(\\alpha+\\beta-1)+1}}\\\\ \u0026=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\mathcal{L}\\left\\{x^{\\alpha+\\beta-1}\\right\\} \\end{aligned} \\end{equation} $$ 这时令$x=1$,便可以得到$\\mathrm{~B}(\\alpha, \\beta)=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$,证毕。\n现在我们使用Beta函数来计算后验分布,注意,如果令$\\alpha = x+1,\\beta = n-x+1$,后验分布的贝叶斯因子为$B(\\alpha,\\beta)=B(x+1,n-x+1)$,那么有\n$$ \\begin{equation} \\begin{aligned} \\int_0^1 \\theta^x(1-\\theta)^{n-x} d \\theta \u0026 =B(x+1, n-x+1) \\\\ \u0026 =\\frac{\\Gamma(x+1) \\Gamma(n-x+1)}{\\Gamma(n+2)} \\\\ \u0026 =\\frac{x!(n-x)!}{(n+1)!} \\end{aligned} \\end{equation} $$ 则我们计算出来的后验分布最终形式为 $$ \\begin{equation} P(\\theta, x)=\\frac{(n+1)!}{x!(n-x)!} \\theta^x(1-\\theta)^{n-x} \\end{equation} $$ 可见该后验分布实际上是服从Beta分布的。根据MSE意义下的最优估计,有\n$$ \\begin{equation} \\begin{aligned} E[\\theta \\mid x] \u0026 =\\int_0^1 \\theta \\frac{(n+1)!}{x!(n-x)!} \\theta^x(1-\\theta)^{n-x} d \\theta \\\\ \u0026 =\\frac{(n+1)!}{x!(n-x)!} \\int_0^1 \\theta^{x+1}(1-\\theta)^{n-x} d \\theta \\\\ \u0026 =\\frac{(n+1)!}{x!(n-x)!} B(x+2, n-x+1) \\\\ \u0026 =\\frac{(n+1)!}{n!(n-x)!} \\frac{(x+1)!(n-x)!}{(n+2)!} \\\\ \u0026 =\\frac{x+1}{n+2} \\end{aligned} \\end{equation} $$ 至此,我们求得了后验分布和MSE意义下的模型最优估计。 如果再进一步的处理该条件期望,我们可以得到线型凸组合的形式： $$ E[\\theta \\mid x]=\\frac{n}{n+2}\\frac{x}{n}+\\frac{2}{n+2}\\frac{1}{2} $$ $\\frac{x}{n}$反应了似然意义上的某种均值,$\\frac{1}{2}$为先验分布的均值,这也显示了在伯努利实验的情况下,最优估计的过程是不断线性凸组合的过程。\nExample:Conjugate prior\n在上个例子中,我们一开始假设先验分布服从标准均匀分布,但是我们根据伯努利的似然求出后验分布之后,发现后验分布并不服从标准均匀分布,而是服从Beta分布,这说明我们的先验分布并没有反映事物的客观性质,在统计学中,如果我们的先验分布和后验分布的分布类型一致,我们就称该先验分布为共轭先验。\n在一些比较简单的概率模型中,共轭先验是已知的并且可以提前制定的,例如当似然服从伯努利分布时,共轭先验为Beta分布,但是更多的(绝大多数的)概率模型的共轭先验是难以得到的。我们沿用上一个例子的情景,令先验分布为Beta分布,即 $$ P(\\theta)=\\frac{(a+b+1)!}{(a-1)!(b-1)!}\\theta^{a-1}(1-\\theta)^{b-1} $$ 使用与上一个例子同样的方法去求后验分布和条件期望,最终可以求得条件期望为 $$ E[\\theta \\mid x] = \\frac{x+a}{n+a+b} $$ 同样的,该条件期望也可以写成线型图组合的形式,即 $$ E[\\theta \\mid x]=\\frac{a+b}{n+a+b}\\frac{a}{a+b}+\\frac{n}{n+a+b}\\frac{x}{n} $$ 依然是某种实验中的均值与先验分布均值的线性凸组合,从我们在上一个章节中的Gauss分布似然到这个章节的伯努利分布似然,最后都得到了线性凸组合的学习形式,这可能并不是一种偶然,这些简单的例子给我们启发：在高维空间中,复杂分布的学习过程也可能是迭代的线性凸组合。\nSampling 使用Bayes方法的核心在于后验分布的处理，但是后验分布的形式往往是极其复杂的，极大多数的后验分布都是难以去积分甚至不可积的，统计中常用的方法就是生成符合后验分布的样本，从而利用样本代替解析表达的分布进行计算，这种生成样本的方法又被称作采样。\n模拟是指把某一现实的或抽象的系统的某种特征或部分状态， 用另一系统（称为模拟模型）来代替或近似。 为了解决某问题， 把它变成一个概率模型的求解问题， 然后产生符合模型的大量随机数， 对产生的随机数进行分析从而求解问题， 这种方法叫做随机模拟方法， 又称为蒙特卡洛(Monte Carlo)方法。\nInverse distribution function 逆分布采样的描述很简单，目标生成随机变量$X\\sim F(X)$的样本($F(x)$为累计分布函数)，只需要生成随机样本$Y\\sim F^{-1}(U)$即可，$Y$即想要得到的分布，$U$为服从标准正态分布的随机变量。\n这样来看，生成随机变量的样本的过程就很简单，第一步，生成均匀分布的样本(可以利用线性同余法等方法生成)，第二步，求得目标分布累积分布函数的逆，第三步，代入累积分布函数的逆求得目标样本。\n现在简单的证明一下随机变量$Y$与随机变量$X$服从同一个分布，求$Y$的累积分布函数： $$ \\begin{equation} \\begin{aligned} F_Y(y)\u0026amp; =P(y\\leq Y)\\ \u0026amp; =P(F^{-1}(u)\\leq Y)\\ \u0026amp; =P(u\\leq F(Y))\\ \u0026amp; =F(Y) \\end{aligned} \\end{equation} $$ 可见，$Y$的累积分布函数实际上就是$X$的累积分布函数，所以它们两个是同一个随机变量，如此，对于易于求得逆累积分布的随机变量，我们便可以通过这种方法求得符合该分布的样本，例如指数分布、泊松分布。\nExample:Sampling for Gaussian Distribution Guassian分布的逆分布函数并不好求，虽然我们可以通过中心极限定理生成Gaussian分布，但那种方法并不属于逆分布生成，Box-Muller算法是一种经典的Gaussian分布生成算法，使用了极坐标变换来处理Gaussian分布难以求逆的问题，接下来我们介绍这种方法。\n假设我们目标求得两个独立同分布的标准高斯随机变量，$X_1,X_2\\overset{\\mathrm{iid}}{\\sim} N(0,1)$，则它们的联合概率密度为 $$ \\begin{equation} \\begin{aligned} f_{X Y}(x, y) \u0026amp; =\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{x^2}{2 \\sigma^2}\\right) \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{y^2}{2 \\sigma^2}\\right) \\ \u0026amp; =\\frac{1}{2 \\pi } \\exp \\left(-\\frac{x^2+y^2}{2 }\\right) \\end{aligned} \\end{equation} $$ 对该联合概率密度进行极坐标变换，有 $$ \\begin{equation} \\begin{aligned} f_{X Y}(x, y) \u0026amp;=f_{XY}(\\rho \\cos\\theta,\\rho \\sin\\theta)\\ \u0026amp;=\\frac{1}{2 \\pi } \\exp \\left(-\\frac{\\rho^2}{2 }\\right)\\rho \\end{aligned} \\end{equation} $$\nAccept-reject sampling Importance sampling Markov chain Monte Carlo(MCMC) ","date":"2024-08-14T20:00:00+08:00","image":"http://localhost:1313/post_file/Bayesian_signal_processing/cover.png","permalink":"http://localhost:1313/p/bayesian-signal-processing/","title":"Bayesian signal processing"},{"content":"Correlation operation and random signal processing My midterm paper on probability theory and stochastic processes. This paper describes the concept of correlation operation, mainly introduces the application of correlation operation in random signal processing, including stationary random signal, blind source separation, template matching filter. You can read my course papers in this link:Correlation operation and random signal processing.\n","date":"2024-08-07T23:30:00+08:00","image":"http://localhost:1313/post_file/Correlation_operation_and_random_signal_processing/cover.png","permalink":"http://localhost:1313/p/correlation-operation-and-random-signal-processing/","title":"Correlation operation and random signal processing"},{"content":"A non-uniform node finite difference algorithm Final design of calculation method course in the first semester of sophomore year. Based on the finite difference method , I proposed an ODE algorithm of non-uniform finite difference , and verified by Matlab simulation, the accuracy of the solution is improved by 10% . You can find this course paper in this pdf : A non-uniform node finite difference algorithm.\nIn addition , I made an academic poster for a simulated academic conference in the college.You can find in this pdf : Poster.\n","date":"2024-08-07T00:22:00+08:00","image":"http://localhost:1313/post_file/non_uni_ode/cover1.png","permalink":"http://localhost:1313/p/a-non-uniform-node-finite-difference-algorithm/","title":"A non-uniform node finite difference algorithm"},{"content":"SMLD and SDE for diffusion model Lab\u0026rsquo;s group meeting on August 1 . I summarized two articles about diffusion model by Song Yang:SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS and Generative Modeling by Estimating Gradients of the Data Distribution . For detailed information, see the pdf of this tutorial:SMLD and SDE.\n","date":"2024-08-06T16:50:00+08:00","image":"http://localhost:1313/post_file/SMLD_and_SDE/cover1.png","permalink":"http://localhost:1313/p/smld-and-sde-for-diffusion-model/","title":"SMLD and SDE for diffusion model"},{"content":"Tutorial of Markdown Basic grammar 引用\n列表 列表 列表 列表 有序列表 1 2 嵌套 嵌套 TodoList a b c 表格 左对齐 右对齐 居中对齐 a b c 段落 分隔线 字体 表格 代码 斜体 * * ==高亮== == == 粗体 ** ** 斜粗体 *** *** 删除 ~~ ~~ 下划线 \u0026lt;u\u0026gt;\u0026lt;/u\u0026gt; 脚注1 代码操作 printf(\u0026quot;hello world!\u0026quot;)\n1 2 code block hello 超链接 github\n图片插入 图床：https://imgse.com/\n直接在图床里粘贴,有链接： 数学公式\n与latex一致\n$ q\\left(x_{t-1} \\mid x_t\\right)=\\frac{q\\left(x_t \\mid x_{t-1}\\right) q\\left(x_{t-1}\\right)}{q\\left(x_t\\right)} $ $$ \\begin{equation} q\\left(x_{t-1} \\mid x_t\\right)=\\frac{q\\left(x_t \\mid x_{t-1}\\right) q\\left(x_{t-1}\\right)}{q\\left(x_t\\right)} \\end{equation} $$\n脚注内容\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-05T12:14:10+08:00","image":"http://localhost:1313/post_file/Markdown/cover.png","permalink":"http://localhost:1313/p/tutorial-of-markdown/","title":"Tutorial of Markdown"}]